<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 云 | 聂同学]]></title>
  <link href="http://www.nietongxue.xyz/blog/categories/yun/atom.xml" rel="self"/>
  <link href="http://www.nietongxue.xyz/"/>
  <updated>2016-03-13T21:24:53+08:00</updated>
  <id>http://www.nietongxue.xyz/</id>
  <author>
    <name><![CDATA[nielinjie]]></name>
    <email><![CDATA[nielinjie@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（六）]]></title>
    <link href="http://www.nietongxue.xyz/blog/2015/06/07/cloud6/"/>
    <updated>2015-06-07T11:15:31+08:00</updated>
    <id>http://www.nietongxue.xyz/blog/2015/06/07/cloud6</id>
    <content type="html"><![CDATA[<p>（<a href="/blog/2015/06/07/cloud5/">接前文</a>）</p>

<h2>分布式系统指南</h2>

<p>当我们开始构建由微服务组成的分布式系统，我们需要应对单体架构系统一般不会需要的非功能需求。有时候需要跟物理定律周旋，比如一致性、延迟、网络分割等。然而另一些问题比如脆弱和可管理性就可以用相对通用的模式来解决。下面我们将介绍这方面的一些实践。</p>

<!--more-->


<p>这些实践来自于<a href="http://projects.spring.io/spring-cloud/">Spring Cloud</a> 和<a href="http://netflix.github.io">Netflix OSS</a> 系列项目的组合使用。</p>

<h3>有版本的分布式配置</h3>

<p>我们已经讨论了<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>适当的配置管理机制的重要性，提到过配置通过操作系统级别的环境变量注入。这种方式非常适合简单系统。但当系统复杂性增加，我们可能需要更多的配置功能，比如：</p>

<ul>
<li>为正在运行的应用改变日志级别，以便诊断生产问题。</li>
<li>改变接收消息的线程数。</li>
<li>报告所有的配置更改，支持生产系统的监管审计。</li>
<li>为正在运行的应用开关某个功能。</li>
<li>支持配置中的保密内容，比如密码。</li>
</ul>


<p>为了支持这些能力，我们的配置机制需要有如下的特性：</p>

<ul>
<li>有版本</li>
<li>可以审计</li>
<li>加密</li>
<li>刷新不需重启</li>
</ul>


<p>Spring Cloud项目中有个<a href="http://cloud.spring.io/spring-cloud-config/">配置服务器</a>支持这些特性。这个配置服务器保存了应用的配置文件，后台是一个git仓库，提供一套REST API（图3-1）。</p>

<p><img src="/images/cloud/springConfig.png" title="[图3-1. The Spring Cloud Config Server]" ></p>

<p>图3-1. The Spring Cloud Config Server</p>

<p>剩下的问题是如何可以不重启应用客户端修改配置。这个能力由Spring Cloud的另一个模块——<a href="http://cloud.spring.io/spring-cloud-bus/">总线</a>提供。这个模块用一个轻量级的消息中间件连接分布式系统中的各节点。它可以用来广播状态变化，比如我们的配置更改（图3-2）。
只要简单地向连入总线的任何应用的<code>/bus/refresh</code>地址发送一个HTTP POST，我们就可以提示所有连入的应用更新他们的配置值，通常更新到配置服务器上的最新值。</p>

<p><img src="/images/cloud/springBus.png" title="[图3-2. The Spring Cloud Bus]" ></p>

<p>图3-2. The Spring Cloud Bus</p>

<h3>服务注册和发现</h3>

<p>当我们建立分布式系统，我们的代码和它的依赖之间就必须通过网络沟通。我们如何有效地将我们的微服务联系起来呢？</p>

<p>云中的一种常见的架构模式（图3-3）是建立前端（应用）服务和后端（业务）服务，后端服务通常不会被直接访问，而是通过前端服务间接访问。服务注册表存放了所有服务的信息，前端服务中有一个客户端库<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>可以到这些信息，从而可以处理路由和负载均衡事务。</p>

<p><img src="/images/cloud/serviceRegistration.png" title="[图3-3. Service registration and discovery]" ></p>

<p>图3-3. Service registration and discovery</p>

<p>我们使用各种<a href="https://en.wikipedia.org/wiki/Service_locator_pattern">Service Locator</a>和<a href="https://en.wikipedia.org/wiki/Dependency_injection">Dependency Injection</a>解决这个问题，面向服务架构长期使用各种服务注册机制。这里我们使用一个类似的方案：<a href="https://github.com/Netflix/eureka">Eureka</a>——来自于Netflix OSS项目，可以在服务定位的同时处理中间层服务的负载均衡、failover机制。<a href="http://cloud.spring.io/spring-cloud-netflix/">Spring Cloud Netflix</a>项目进一步简化了对Eureka的使用，它提供了一个基于Annotation的配置模型。</p>

<p>所有使用<a href="http://projects.spring.io/spring-boot/">Spring Boot</a>的应用都可以通过简单添加<code>@EnableDiscoveryClient</code>来获得服务注册和发现功能。</p>

<h3><a name="rlb"></a>路由和负载均衡</h3>

<p>简单的循环负载均衡在很多场景下是很有效的。但在云环境下的分布式系统需要进一步的路由与负载均衡行为。以前这通常由外部的集中的负载均衡服务提供。然而这种服务往往没有足够的信息和上下文，它们也没办法为应用提供最佳选择。同时，集中的解决方案存在单点失效的问题，当他们出问题了整个架构都要受到影响。</p>

<p>云架构把路由和负载均衡的职责转移到客户端。这种方案的一个例子是来自Netflix OSS项目的<a href="https://github.com/Netflix/ribbon">Ribbon</a>（图3-4）。</p>

<p><img src="/images/cloud/clientLoadBalancer.png" title="[图3-4. Ribbon client-side load balancer]" ></p>

<p>图3-4. Ribbon client-side load balancer</p>

<p>Ribbon提供了丰富的功能：</p>

<ul>
<li>内建多种负载均衡规则

<ul>
<li>循环</li>
<li>平均响应时间加权的循环</li>
<li>随机</li>
<li>可用性过滤（避免tripped circuits和大并发连接数）</li>
</ul>
</li>
<li>定制均衡规则插件</li>
<li>与服务发现方案（包括Eureka）的可拔插集成</li>
<li>Cloud-native intelligence such as zone affinity and unhealthy zone avoidance</li>
<li>内建错误容忍</li>
</ul>


<p>就跟Eureka类似，Spring Cloud Netflix 项目进一步简化了对Ribbon的使用，将注入<code>DiscoveryClient</code>变为注入<code>LoadBalancerClient</code>，就可以从直接使用Eureka切换为使用Ribbon</p>

<h3><a name="ft"></a>错误容忍</h3>

<p>分布式系统潜在的错误比单体系统要多。现在每一个请求都需要使用数十个甚至上百个不同的微服务，其中的一个或多个出问题几乎是肯定的。</p>

<blockquote><p>如果不采取必要地错误容忍措施，30个依赖服务的系统每个月将有两个小时多的宕机时间，即使每个服务可用性都是99.99%。（99.99%^30 = 99.7% uptime = 2+ hours in a month）—<em>Ben Christensen, Netflix Engineer</em></p></blockquote>

<p>我们如何防止类似的错误堆积呢？</p>

<p>Mike Nygard论述<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>了几种有用的模式：</p>

<ul>
<li><a name="bc"></a>断流器<br/>
断流器会隔离一个服务，如果发现它的依赖服务状态不佳的话。断流器通常被实现为一个状态机（图3-5）。当它处于闭合状态，调用就简单地被传递到依赖。如果调用失败了，断流器开始对失败计数，当在特定时间内失败次数达到了特定值，断流器就切换到断开状态。当断流器处于断开状态，任何调用都会立即返回，根本不会真正尝试调用依赖。再过了一个特地时间过后，断流器切换到半开状态。在半开状态，调用会被传递到依赖，如果成功，断流器切换到闭合状态，否则切到断开状态。</li>
</ul>


<p><img src="/images/cloud/stateMachine.png" title="[图3-5. A circuit breaker state machine]" ></p>

<p>图3-5. A circuit breaker state machine</p>

<ul>
<li>隔板<br/>
隔板分割服务，防止整个服务因为局部错误而整体失效。软件系统可以从多个层面应用隔板。简单地分割为微服务就是第一种应用。将应用进程分割为Linux容器<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>，使得单个进程不会影响整个机器，也是一种应用。还有个例子是将并行运算分散到多个线程池中。</li>
</ul>


<p>Netflix在库<a href="https://github.com/Netflix/Hystrix">Hystrix</a>中加入了很强大错误容忍功能。Hystrix通过<code>HystrixCommand</code>来将代码包装在断流器中。</p>

<p>Spring Cloud Netflix项目通过<code>@EnableCircuitBreaker</code>注解，在Spring Boot应用中加入Hystrix部件。并且借助一系列<a href="https://github.com/Netflix/Hystrix/tree/master/hystrix-contrib/hystrix-javanica">捐献的注解</a>使得编程更加的简单。</p>

<p>Hystrix不同于其他的断流器，它还应用隔板模式，将各个断流器封装在各自的线程池中。它还收集一些有用的数据：</p>

<ul>
<li>流量</li>
<li>请求速率</li>
<li>错误占比</li>
<li>Hosts reporting</li>
<li>延迟分布</li>
<li>请求结果：成功、失败、拒绝</li>
</ul>


<p>这些数据作为事件流散发，可以用另一个Netflix OSS工具—— <a href="https://github.com/Netflix/Turbine">Turbine</a>进行综合。单独的综合的数据都可以通过一个Hystrix面板展示出来（图3-6），面板对分布式系统的健康状况提供了很好地可视化。</p>

<p><img src="/images/cloud/dashboard.png" title="[图3-6. Hystrix Dashboard showing three sets of circuit breaker metrics]" ></p>

<p>图3-6. Hystrix Dashboard showing three sets of circuit breaker metrics</p>

<h3><a name="api"></a>API网关和边缘服务</h3>

<p>我们已经提到了<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>微服务中服务方的服务整合，我们来具体看看它的必要性：</p>

<ul>
<li>延迟<br/>
移动端通常使用比较慢速的网络，如果应用需要依次跟几十几百个服务通信，那延迟是很难接受的。容易理解并行发起请求是很有必要的。相比于在各个不同的移动端平台上实现并行模式，在服务端实现会比较便宜也不容易出错。</li>
<li>续航<br/>
就算网络速度不是问题，客户端跟大量微服务打交道还是会有问题。使用网络对于移动端来说是很消耗电池电量的。移动开发者通常都希望减少与服务端的通信来增强用户体验。</li>
<li>设备多样性<br/>
移动端的设备多样性非常明显。比如厂商、尺寸、操作系统、编程语言等等都有很多不同。</li>
</ul>


<p><a href="http://microservices.io/patterns/apigateway.html">API网关</a>模式就是将移动端开发的负担转移到服务端。API网关就是一个普通的微服务，只不过它是与单个移动应用对应的，为它提供单一的后台入口。它每个请求都会跟几十几百个服务并行通信，将所有返回结果综合起来再返回到移动端。如果有必要，它也处理协议翻译的工作，比如HTTP翻译为AMQP。</p>

<p><img src="/images/cloud/apiGateway.png" title="[Figure 3-7. The API Gateway pattern]" ></p>

<p>Figure 3-7. The API Gateway pattern</p>

<p>API网关可以用任何语言、运行时或者框架实现，只要它能支持web编程、并发以及跟后台服务通信的协议。流行的选择包括Nodejs（有reactive编程模型）和Go（有简单的并发模型）。</p>

<p>如果使用Java，可以考虑<a href="https://github.com/ReactiveX/RxJava">RxJava</a>， <a href="http://reactivex.io">Reactive Extensions</a> 的Java实现。毕竟如果只使用Java提供的原生特性，合并并行处理的结果这一点很难做好。</p>

<h2>总结</h2>

<p>以下又是译者自己总结的 :-)</p>

<ul>
<li>分解： 新特性作为微服务、防腐层、扼杀单体</li>
<li>分布式系统： 配置服务和管理总线、动态服务发现、去中心化的负载均衡、断流器和隔板、API网关</li>
</ul>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="/blog/2015/05/23/cloud2/#12f">十二因子应用</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><a href="#rlb">路由和负载均衡</a><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>书：<em>Release It!</em><a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p><a href="/blog/2015/06/02/cloud4/#c">容器化</a><a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p><a href="/blog/2015/05/23/cloud/#mc">移动应用和客户端多样性</a><a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（五）]]></title>
    <link href="http://www.nietongxue.xyz/blog/2015/06/07/cloud5/"/>
    <updated>2015-06-07T11:15:31+08:00</updated>
    <id>http://www.nietongxue.xyz/blog/2015/06/07/cloud5</id>
    <content type="html"><![CDATA[<p>（<a href="/blog/2015/06/02/cloud4/">接前文</a>）</p>

<h1>迁移指南</h1>

<p>现在我们已经定义了云架构，简短讨论了企业如果要使用云架构需要些什么转变。现在我们谈谈具体的技术。这里更多的是提供简短介绍和进一步阅读的链接。</p>

<!--more-->


<h2>分解指南</h2>

<p>我们已经讨论了分解数据、服务、团队，那么，我们怎么才能实现这些呢？好问题，我们如何打破现存的巨大的一切，前进到云架构呢？</p>

<p>我已经看到一些公司成功地进行了渐进式迁移，有一些可以借鉴的模式。这里有一些公开的例子：<a href="https://blog.yourkarma.com/building-microservices-at-karma">Karma</a>、<a href="https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith">SoundCloud</a>。</p>

<p>下面我们将一步一步看看如何将单体上的服务迁移到云。</p>

<h3>新特性作为微服务</h3>

<p>有点意外的是，第一步并不是打碎单体。我们现在假设在单体上你有一些待实现的特性。事实上，如果你的单体根本没打算实现新功能，那恐怕根本没有必要去分解它。我们首要的目的是快速变化，如果你的系统根本没打算要变化，谈何快速变化呢？</p>

<blockquote><p>团队认为改变架构的好办法并不是立即分拆单体，而是不再在上面增加任何新东西，新东西都建立为微服务——<em>Phil Calcado, SoundCloud</em></p></blockquote>

<p>这个策略显然的好处就是我们建立新的微服务的难度很低，毕竟从头开始建立要比从一个<a href="http://www.laputan.org/mud/">大泥球</a>上动刀要容易得多。</p>

<p>当然不可避免的，我们的微服务需要跟单体沟通，我们如何处理这个问题呢？</p>

<h3>反腐层</h3>

<blockquote><p>因为我们的逻辑还在Rails写的单体中，所有的微服务都还得这样或那样地跟它沟通。——<em>Phil Calcado, SoundCloud</em></p></blockquote>

<p>DDD讨论了<em>反腐层</em>，它的目的是让两个不同领域模型的系统可以相互沟通，而不必让一个系统的领域模型同化另一个。当你在新的微服务中实现新的功能的时候，你肯定不希望新功能耦合在既有单体的内部业务知识上。反腐层是一种建立API合同的办法，让单体看起来就像是另一个微服务。</p>

<p>Evans 将反腐层的实现划分为三个子层：</p>

<ul>
<li>Facade —— 在这里Facade的目的是简单地集成单体的接口，因为很可能单体的接口并不是按照现行集成原则来设计的。重要的是，这里不要改变单体的领域模型，不要把集成和翻译耦合在一起了。</li>
<li>Adapter —— Adapter中我们定义我们需要的“服务”。Adapter知道如何从我们的系统接收一个请求，然后将请求转发给单体。</li>
<li>Translator —— Translator的任务就是为我们的服务和单体之间的请求和响应转换领域模型。</li>
</ul>


<p>这三个松耦合的部件解决三个问题：</p>

<ul>
<li>系统集成</li>
<li>协议翻译</li>
<li>模型翻译</li>
</ul>


<p>现在剩下的问题是在哪一层进行通信。DDD中Evans讨论了两种办法：第一种，<em>facade to system</em>，当你无法改变遗留系统的时候使用。在这里我们假定我们是可以修改遗留单体的，那么我们主要讨论第二种，<em>adapter to facade</em>，也就是我们将Facade放进到遗留单体中，所以通信在Adapter和Facade之间进行。因为显然两个专门现写的部件通信起来容易些。</p>

<p>最后值得注意的是，防腐层也能促进双向沟通，毕竟遗留单体有时候也是需要跟新建立的微服务沟通的。</p>

<h3>扼杀单体</h3>

<blockquote><p>随着架构变化的进行，团队得以在一个灵活得多的环境里构建新特性和改进，剩下的问题是，我们如何从单体中提取已有特性？——<em>Phil Calcado, SoundCloud</em></p></blockquote>

<p>我从Martin Fowler的文章<a href="http://www.martinfowler.com/bliki/StranglerApplication.html">StranglerApplication</a>中借用了“扼杀单体”的说法。在这篇文章中Fowler介绍：在老系统的周边逐渐建立一个新系统，让老系统生长缓慢，几年后最终被扼杀。我们要做的正是如此：通过一系列的微服务和反腐层，我们在老系统周边逐渐建立新的云中系统。</p>

<p>两个原则帮助我们选择要提取的特性：</p>

<ul>
<li>SoundCloud制订了第一个原则：在单体中识别界限上下文。正如我们前面讨论的，界限上下文要求内部一致的领域模型。遗留单体的领域模型显然不会是内部一致的，现在我们就开始从中识别内部一致的子模型。这些就是我们提取的候选。</li>
<li>第二个原则讨论了优先级：哪些候选先提取？我们可以通过回顾迁移到云的意义来回答这个问题：快速创新。哪个特性提取后最有助于快速创新？我们显然选择那些些现有业务需要它快速变化的特性。</li>
</ul>


<h3>怎么才算完成？</h3>

<p>我们怎么知道我们完成了？一般有两个完成标志：</p>

<ul>
<li>单体完全被扼杀了。所有的界限上下文都被提取成了微服务。最后一步就是识别出并干掉那些不再需要的防腐层。</li>
<li>单体被扼杀到了一定程度：要继续提取特征的话，成本已经超过了收益。有时候单体的一部分很稳定，它们几年都不变，工作得也很好。那么提取这些部分就没有什么价值，如果同时维护相关的反腐层成本也够低，那我们就让它们长期保留好了。</li>
</ul>


<p>（<a href="/blog/2015/06/07/cloud6/">后文继续，分布式系统指南</a>）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（四）]]></title>
    <link href="http://www.nietongxue.xyz/blog/2015/06/02/cloud4/"/>
    <updated>2015-06-02T11:15:31+08:00</updated>
    <id>http://www.nietongxue.xyz/blog/2015/06/02/cloud4</id>
    <content type="html"><![CDATA[<p>（<a href="/blog/2015/05/29/cloud3/">接前文</a>）</p>

<h2>组织转变</h2>

<p>这一节我们将讨论为了更好地应用云架构，组织在建立团队方面将如何改进。这个理论的背后是著名的<em>Conway’s Law</em>，我们的办法是围绕一个长期的产品线建立多职责的团队，而不是鼓励每种职责的人员呆在自己单独的团队里（比如testing）。</p>

<!--more-->


<h3><a name="bct"></a>业务能力<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>团队</h3>

<blockquote><p>任何组织设计的产品，它的设计结构都跟组织的交流结构一致。—— Melvyn Conway</p></blockquote>

<p>我们已经讨论了把IT划分为竖井的做法。自然地，这时我们也把每个人放进了这样的竖井里。那么我们来看看需要开发一个新软件时会发生什么。</p>

<p>通常的做法是建立一个项目团队，指定一名项目经理。然后项目经理会跟各个竖井打交道，获得项目所需的各种人员。从上面引用的<em>Conway’s Law</em>我们会看到，这种团队自然就产出了类似竖井结构的架构：</p>

<ul>
<li>数据操作层</li>
<li>服务层</li>
<li>Web MVC 层</li>
<li>消息层</li>
<li>……</li>
</ul>


<p>这些层次横亘在各个业务能力间，各个业务能力要想不影响其他地创新和落地，变得比较困难。</p>

<p>很多公司想要迁移到云架构，比如按业务能力划分微服务。他们应用的是Thoughtworks所谓的
<a href="http://www.thoughtworks.com/radar/techniques/inverse-conway-maneuver">“反Conway策略”</a>。不是按组织结构来决定架构，而是反过来，按照想要的架构来重新调整组织架构。按照Conway的观点，只有这样，你想要的架构才会最终出现。</p>

<p>所以，作为迁移到DevOps文化的一部分，团队跨职责并按照业务能力来组织，他们开发产品而不是项目。产品是长期存在的，直到他们不再具有业务价值。交付一个业务能力需要的人员都在一个团队中——开发、测试、发布、运营，代码不需要在各个团队间传来传去。这种团队往往也叫“<a href="http://www.fastcompany.com/50106/inside-mind-jeff-bezos">两个披萨团队</a>”，也就是说如果两个披萨不够这个团队分，那这个团队就太大了点。</p>

<p>现在我们来看如何决定要建立什么团队。如果我们遵从“反Conway策略”，我们将从组织的领域模型开始，找到能够封装在“界限上下文”<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>里地业务能力。一旦确定了这些业务能力，我们就建立相关的业务能力团队，负责这个业务能力的整个生命周期，同时也负责从开发到运营的整个循环。</p>

<h3><a name="pt"></a>平台团队</h3>

<p>业务能力团队依赖于“敏捷的自助基础设施”<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>。实际上，我们可以定义一个业务能力，叫做“可以开发、部署、运营业务能力的业务能力”。这个能力由平台团队负责。</p>

<p>平台团队根据业务能力团队的要求运营着“敏捷的自助基础设施”。如果公司自己运行云平台，那平台团队需要负责数据中心和了解硬件。</p>

<p>IT运营往往通过各种ticket系统与客户交互。但平台团队运营的是“自助”的平台，它要以不同的方式交互。就像业务能力团队通过API合同跟其他团队协作，平台团队也为平台定义一套合同。业务能力团队不是将环境和数据请求放到等待队列等待实施，而是请求并获得一个自动发布管道，需要的时候能够自行处理环境和数据。</p>

<h2>技术转变</h2>

<p>现在我们来看看迁移到云中的DevOps可能遇到的实现问题。</p>

<h3>分解单体<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>架构</h3>

<p>传统的n层单体架构部署到云基础设施很难运行得好。因为它们往往就运行环境做了一些假设，然而云基础设施并不支持，比如：</p>

<ul>
<li>mount好的共享文件系统</li>
<li>P2p的应用服务集群</li>
<li>共享运行库</li>
<li>已知位置的配置文件</li>
</ul>


<p>这些假设都基于这种架构的应用被部署在长期存在的基础设施上，它们跟云基础设施弹性短期的想法并不兼容。如果应用并没有这些环境假设呢？还是有问题：</p>

<ul>
<li>单体架构将所有业务能力的变化绑在一起，不利于各个业务能力的创新分别落地。妨害了创新的快速。</li>
<li>单体架构中的服务很难单独伸缩，负载效率难以提升。</li>
<li>新加入组织的员工难以适应，他们需要学习整个领域和大量的代码，没有几个月的时间，他们很难成为真正有效率的开发人员。</li>
<li>开发组织通过增加人员来扩大规模很难，增加沟通和协作的成本。</li>
<li>技术栈长期不变，引入新技术风险较大。</li>
</ul>


<p>以上清单正好是“微服务”<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>的优势清单逐条反过来（<s>我Kao</s>）。同时，将组织划分为业务能力团队，也要求将单体架构分解为微服务。只有这样，才能充分享有我们迁移到云基础设施的好处。</p>

<h3><a name="dd"></a>分解数据</h3>

<p>分解单体架构并不足够。数据模型也需要分解。如果业务能力团队看似自治但却被限制在同一个数据上协作，单体架构对创新的阻碍只不过搬到了数据层面而已。</p>

<p>DDD认为<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>我们的成功很大程度上依赖于我们的领域模型的质量。领域模型只有内部一致才能有用，同一个模型中不应该有不一致的概念和词汇。</p>

<p>要产出一个大一统的领域模型非常困难和昂贵，甚至是不可能的。Evans根据内部一致的子集来划分领域，称为“界限上下文”。</p>

<p>最近在跟航空业的客户合作时，我们讨论了几个他们业务的核心概念。比如“预定”，我们能在相关业务中找出十七个不同的定义，显然它们不能看做是同一个概念。相反，每个定义都是有细微差异的不同概念。这成为了组织的瓶颈。</p>

<p>界限上下文允许我们在组织范围保留不一致的定义，同时在单个上下文里面保持一致。</p>

<p>我们开始识别哪些能够内部一致的领域模型片段。我们在这些片段的周围画出边界，这就成了我们界限上下文。这样我们就能把我们的业务能力团队对应到界限上下文，这些团队将产出对应的微服务。</p>

<p>微服务的定义又指导了需要哪些十二因子应用。十二因子主要是技术上的规范，而微服务主要是业务上的规范。我们定义界限上下文，赋予之业务能力，围绕业务能力建立团队，让团队开发十二因子应用。由于这样的应用都是独立可部署的，使得业务能力团队有更多的技术手段可用。</p>

<p>我们将界限上下文与“每服务每数据库”模式关联，也就是每个微服务封装、管理和保护它们自己的领域模型和数据存储。在这个模式中，只有一个应用服务可以访问它的数据存储（可能是一个多租户数据库集群中的一个schema，也可以是一个独占的物理数据库）。任何外部访问只能通过API合同（经常是REST，但是可以是任何协议）。</p>

<p>这种分解使应用可以根据自己的数据特征选择不同的数据存储，比如数据结构和数据读写模式。另一方面，为了能回答一些跨上下文的问题，数据通过事件驱动技术重新组合起来。比如<a href="http://martinfowler.com/bliki/CQRS.html">CQRC</a>和<a href="http://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a>就常常用来实现跨上下文将相似概念同步起来。</p>

<h3><a name="c"></a>容器化</h3>

<p>容器镜像（比如由<a href="https://linuxcontainers.org">LXC</a>、<a href="https://www.docker.com">Docker</a>、<a href="https://github.com/coreos/rkt">Rocket</a>准备的）正在迅速成为云架构的部署单元。这些镜像也迅速得到了调度工具的支持，比如Kubernetes、Marathon、 Lattice。公有云提供商比如Amazon和Google也提供了基于容器的调度和部署服务。容器技术提供了跟虚拟机相似的资源分配和隔离，同时相对来说大大地轻量和易移植。应用开发者要尽快适应将应用打包为容器镜像，以便能享有现代云基础设施带来的便利。</p>

<h3>从奏乐到舞蹈<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup></h3>

<p>不仅仅服务产出、数据模型和管理应该去中心化，服务的集成也应该去中心化。企业中服务集成传统上使用类似ESB。ESB就成为了服务间交互的所有决策的控制者，包括路由、传输、协议、安全等。我们称为“奏乐”，类似于指挥决定了整个音乐的演奏进程。ESB和奏乐模式使得架构图显得非常简单，但不幸的是这种简单性具有欺骗性。ESB中经常隐藏着一张复杂性的网。管理这种复杂性非常费时，跟它一起工作也成为了应用开发团队的瓶颈。就像我们谈到的大一统的数据模型一样，大一统的服务集成也成为了追求快速的绊脚石。</p>

<p>在云架构中，比如微服务，我们倾向于“舞蹈”，类似于芭蕾舞。不是把精力放在集成机制，而是放在各个节点。当舞台上发生了跟原计划不符的异常情况，并没有一个指挥来告诉舞者该怎么做，而是舞者自己适应。类似的，我们的服务也是自己适应他们运行环境中出现的异常情况，比如通过“客户端负载均衡<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>”和“断流器<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>”。</p>

<p>虽然架构图看上去趋向于一张纠结的网，但其复杂性并不超过传统的SOA。舞蹈模式只是承认和暴露了系统本质的复杂性。这个转变同样是为了支持自治进而实现云架构快速地目标。团队可以迅速应对各种情况，不必与其他团队过多协作，也不用疲于跟ESB打交道。</p>

<h2>总结</h2>

<p>以下为译者自己总结的 :-)</p>

<ol>
<li>文化转变：DevOps、持续交付、自治</li>
<li>组织结构转变：业务能力团队、平台团队</li>
<li>技术转变：分解服务、分解数据、容器化、分解集成</li>
</ol>


<p>（<a href="/blog/2015/06/07/cloud5/">后文继续，迁移指南</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>business capability<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>bounded contexts，在“<a href="#dd">分解数据</a>”中讨论<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="/blog/2015/05/23/cloud2/#ai">前面已经提到</a>。<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>Monolith<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p><a href="/blog/2015/05/23/cloud2/#ms">微服务</a>。<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>书：<em>Domain-Driven Design</em>，作者Eric Evans<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
<li id="fn:7">
<p>从Orchestration到Choreography<a href="#fnref:7" rev="footnote">&#8617;</a></p></li>
<li id="fn:8">
<p><a href="/blog/2015/05/07/cloud6/#rlb">路由和负载均衡</a><a href="#fnref:8" rev="footnote">&#8617;</a></p></li>
<li id="fn:9">
<p><a href="/blog/2015/06/07/cloud6/#ft">错误容忍</a><a href="#fnref:9" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（三）]]></title>
    <link href="http://www.nietongxue.xyz/blog/2015/05/29/cloud3/"/>
    <updated>2015-05-29T11:15:31+08:00</updated>
    <id>http://www.nietongxue.xyz/blog/2015/05/29/cloud3</id>
    <content type="html"><![CDATA[<p>（<a href="/blog/2015/05/23/cloud2/">接前文</a>）</p>

<h1>需要转变</h1>

<h2>文化转变</h2>

<h3>从竖井到DevOps</h3>

<p>企业IT常常被组织为一些竖井：软件开发、QA、DBA、系统管理、运营、发布管理、项目管理等。这些竖井往往有不同的管理层次、工具集、沟通方式、词汇和激励形式。这些不同鼓励了做事方法的不同。</p>

<!--more-->


<p>一个常被提到的例子是开发和运营的不同。
开发的任务是开发功能，往往是通过向企业IT中引入更改来实现的。所以开发部门的任务可以说成是“交付变化”，自然开发部门也往往是鼓励交付更多的变化。</p>

<p>另一方面，运营的任务则可以说是“防止变化”，运营的任务是保持可用性、弹性、性能和持久性。他们往往被鼓励保持KPI，比如平均错误间隔，平均恢复时间等。然而对于保持这些指标来说，最大的风险就是引入变化。结果就是，运营经常不是想办法怎么安全地引入开发期望的变化，而是想办法把让引入变化更痛苦，从而降低变化的频率。</p>

<p>这种不同的做事方法显然带来了低效的合作。协作、沟通、交接工件变得非常乏味和痛苦，甚至是混乱和危险。企业IT往往企图修复这种状况，构建了通过ticket系统和会议驱动的重流程。结果是工作变得更加慢和浪费了。</p>

<p>像这样的环境跟云架构快速的理念完全相左。竖井诞生的原因，往往是希望带来更多地稳定和安全，但实践证明，作用甚微，甚至有时候会带来反作用。</p>

<p>DevOps的核心就是打破这样的竖井。构建共享的工具、词汇和沟通结构，建立文化，为着同一个目标：快速安全地实现价值。围绕这一目标建立激励机制。官僚机构和流程被信任和责任心取代。</p>

<p>届时，开发和运营向同一个直接领导负责，共同寻找办法，既能持续产出价值，又能保持可用性、弹性、性能和持久性。现在发现，云架构相关技术能够为之提供支持。</p>

<h3>从Punctuated Equilibrium<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> 到持续交付</h3>

<p>企业往往引入了敏捷过程，比如Scrum，但一般限于开发团队。</p>

<p>我们已经相当成功地将单个的开发团队变得敏捷。我们做了很多改进：结对编程、测试驱动开发，持续集成……，我们已经能很高效地完成“从用户故事到Demo”的循环。但我们的问题是：<em>功能什么时候能上生产呢？</em></p>

<p>这个问题我们很难回答，它迫使我们考虑一些我们不能控制的因素：</p>

<ul>
<li>QA流程需要多少时间？</li>
<li>我们啥时候能加入版本计划？</li>
<li>什么时候运营能准备好环境？</li>
</ul>


<p>这时候我们意识到我们陷入了“<a href="http://sdtimes.com/analyst-watch-water-scrum-fall-is-the-reality-of-agile/">Water-Scrum-fall</a>” ，我们的团队已经开始敏捷了，但我们的组织还没有。结果，我们每个迭代的成果并不能体现到生产部署，代码其实还堆积在传统的发布循环中。</p>

<p>这种工作模式实际上抹杀了敏捷的两个关键好处。</p>

<ul>
<li>客户仍然要隔好几个星期才能看到新东西，感觉不到敏捷的好处，所以并不能如约与开发团队建立信任关系。他们倾向于回到老的工作方式：把尽量多的需求都塞到版本里——由于他们不太相信软件能够快速发布，于是宁愿最后好不容易发布的时候能多交付点东西。</li>
<li>开发团队可能要好几周才能得到真正的反馈。Demo是很不错，但开发人员都知道，只有真正的用户使用了真正的生产软件，才能产生好的反馈。这样的反馈才能提供有效的修正，团队才能真正做到“build the right thing”。如果反馈延迟了，错误堆积起来，返工的成本就很高。</li>
</ul>


<p>想要得到云架构的好处就需要转变到持续发布。</p>

<p>技术上我们可以做到每个迭代（甚至每次代码提交）自动部署到生产。我们建立部署管道，在此执行自动测试，防止生产问题。现在唯一需要做的是业务决策：现在是否是发布新特性的好的业务时机？由于部署管道是全自动的，业务决策好后可以一键实施。</p>

<h3>从中央集权到分散自治</h3>

<p>Water-Scrum-fall文化中有一点值得特别指出，因为我感觉是使用云的过程中的一个绊脚石。</p>

<p>企业经常围绕应用架构和数据管理建立中央集权组织，负责维护守则和标准、审批设计和更改。中央集权确实有几点帮助：</p>

<ul>
<li>可以防止技术栈的不一致。降低组织的维护总成本。</li>
<li>可以防止架构选择的不一致。</li>
<li>合规管理之类的Cross-cutting concerns可以在组织内保持一致。</li>
<li>数据的所有权可以一目了然。</li>
</ul>


<p>这些组织建立起来是为了提高质量和降低成本。但收效甚微。同时还影响了云架构在快速方面的努力。单体架构可能成为技术创新的瓶颈，巨大单一的管理组织也一样。有时候一个小的变化只需要几分钟就能完成，而且也肯定会通过评审，但却需要很长时间来等待评审会议的召开。</p>

<p>使用云架构几乎总是伴随着去中心集权。开发云架构应用的团队（“Business Capability Teams”<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>）几乎拥有了交付所需要的所有的能力。他们拥有数据管理、技术栈、架构、每个部件的设计以及API合同。如果有什么决定需要做，那就是这个团队就能做了。</p>

<p>去中心和自治的团队通过集成模式规定的机制来制衡，这些机制存在于团队开发的服务之间，通常很小很轻。（比如使用HTTP REST JSON风格的API，而不是各不相同的RPC接口。）这些机制通常最先出现在基层，用来解决一些共有问题，比如“错误容忍”。我们鼓励各团队自己设计解决方案，然后自组织地与其他团队一起形成公用的模式和框架。如果出现了对整个组织都好用的模式和框架，那么就将这些模式、框架转交给“云工具、框架团队”。工具框架团队可以是平台团队<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>的一部分，也可以不是。在整个团队对架构形成共识的过程中，工具框架团队和他们的方案，将起到带头作用。</p>

<p>（<a href="/blog/2015/06/02/cloud4/">后文继续，组织转变</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://en.wikipedia.org/wiki/Punctuated_equilibrium">Punctuated Equilibrium</a>（什么鬼……）<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><a href="/blog/2015/06/02/cloud4/#bct">业务能力团队</a><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="/blog/2015/06/02/cloud4/#pt">平台团队</a><a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（二）]]></title>
    <link href="http://www.nietongxue.xyz/blog/2015/05/23/cloud2/"/>
    <updated>2015-05-23T11:15:31+08:00</updated>
    <id>http://www.nietongxue.xyz/blog/2015/05/23/cloud2</id>
    <content type="html"><![CDATA[<p>（<a href="/blog/2015/05/23/cloud/">接前文</a>）</p>

<h2>云架构的定义</h2>

<h3><a name="12f"></a>十二因子应用</h3>

<p>“十二因子应用”是一些云架构模式。它们专注于速度、稳定、伸缩，主要包括强调声明性配置、无状态和无共享的横向伸缩进程、全面与部署环境解耦合等方面。当前大量的云平台都为部署十二因子应用优化。</p>

<p>在“十二因子应用”的说法中，“应用”指的是单个部署单元。</p>

<p>十二因子应用表述为：<br/>
（省略，可参见官方中文版：<a href="http://12factor.net/zh_cn/">十二因子应用</a>）</p>

<!--more-->


<p>十二因子让应用可以快速部署，因为对环境没有多少要求。对环境没有要求也让云平台可以采用简单一致的机制来自动化地提供新的环境。这是十二因子应用在速度方面的意义。</p>

<p>十二因子也使得应用满足“生命短促”的要求，换句话说我们可以以很小的成本丢弃这些应用。首先应用环境本身完全是可丢弃的，同时应用的状态，不管内存中的还是持久化的，都外化到支持服务中。这就使得应用可以简单地自动化地伸缩。在大多数情况下，平台只要按想要的数量简单拷贝现有的环境然后启动进程就可以了。当需要缩减的时候，只要关闭运行中的进程，然后删除相应地环境就可以了，完全不用备份或者保护这些环境。这是十二因子应用在伸缩方面的意义。</p>

<p>最后，应用的可丢弃性使得平台自动错误恢复非常容易快速。同时，将日志作为事件流大大地帮助了应用状态的可视化。环境的等同、一致的配置机制以及支持服务机制则使得平台能为应用运行的各个方面提供更强大的可视化能力。这是十二因子应用在稳定方面的意义。</p>

<h3><a name="ms"></a>微服务</h3>

<p>微服务代表了将单体架构<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>业务系统解构为独立可部署的服务。每个服务代表一个业务能力，或者说是产生业务价值的最小原子服务。</p>

<ul>
<li>我们将业务领域解耦为独立可部署的上下文的同时，我们就解耦了相关的变化迭代。当变化被局限在单个的上下文，这些变化可以独立于业务的其他方面。结果就是可以更频繁迅速地落实，从而持续不断地产生价值。</li>
<li>便于加速开发。包括可以投入更多地开发人员。可以将开发人员分开，让他们平行工作，不用过多的交流协作。</li>
<li>新加入的开发人员上手更快，因为他们需要学习的东西更少，需要打交道的团队也更小。</li>
<li>应用新技术可以更容易。在单体架构应用中应用新技术风险较大，一旦犯错可能拖累整个企业架构。</li>
<li>微服务带来独立有效的伸缩。巨大架构应用也可以伸缩，但要求所有部件一起伸缩，不光是哪些负载重的部件。</li>
</ul>


<h3><a name="ai"></a>敏捷的自助基础设施</h3>

<p>云架构应用的部署与维护往往由开发团队负责。支持自助的平台往往有帮助。</p>

<p>好的平台能为用户建立一个抽象层。在IAAS中，我们通过调用API来建立虚拟服务器、网络、存储，然后应用不同形式的配置和自动化来运行我们的应用和支持服务。平台现在鼓励我们以应用和支持服务的角度思考。</p>

<p>应用代码只是简单地被push到git仓库，平台就开始自动编译打包、建立应用环境、部署应用、启动必要地进程。团队不需要知道代码在哪里运行，也不用知道代码是怎么到那里去的。</p>

<p>支持服务也采用类似的机制。不论需要数据库、消息队列还是邮件服务器，只需要告诉平台你的需求。现在的平台大多提供SQL/NoSQL数据库、消息队列、搜索引擎、缓存等各种重要的支持服务。这些服务的实例可以被绑定到我们应用，必要地使用许可被自动注入到应用中。完全不需要繁琐和容易出错的各种定制。</p>

<p>这些平台还提供各种其他能力：</p>

<ul>
<li>应用实例自动化和按需的伸缩。</li>
<li>应用健康管理。</li>
<li>动态路由和负载均衡。</li>
<li>日志和测量数据的汇总。</li>
</ul>


<p>这一系列的工具保障了团队可以敏捷地开发和运营他们的服务，并且做到快速、稳定、可伸缩。</p>

<h3>基于API的协作</h3>

<p>云架构中，服务之间唯一的交互方式是API，公开发布和有版本的API。通常采用HTTP上的REST风格并使用JSON序列化，但其他协议和序列化方式也完全可以。</p>

<p>团队可以在需要的时候部署新的功能。只要他们不破坏任何既有的API合同，就不用跟其他团队协调同步。跟自助基础设施交互的主要方式也是API，不论新建、伸缩还是维护应用基础设施，都通过API的调用进行。</p>

<p>通过<a href="http://martinfowler.com/articles/consumerDrivenContracts.html">客户驱动合同</a>，交互双方校验合同。服务的消费者不允许访问依赖的实现细节，也不能直接访问它的数据存储。事实上，只有一个服务可以访问它自己的数据存储。这种强制解耦合有利于云架构的速度。</p>

<h3><a name="an"></a>“反脆弱”</h3>

<p>“反脆弱”并不是指的鲁棒性或弹性。而是指系统在压力下变得更强的特性。什么系统可以做到这个？比如人的免疫系统，暴露的时候变得更强而隔离的时候变得更弱。比如，Netflix Simian Army项目有个Chaos Monkey子模块，通过向生产部件中随机注入错误来寻找架构中的弱点。通过显式地寻找弱点、注入错误、强制修复，架构自然随时间逐渐变强。</p>

<h2>总结</h2>

<p>以下为译者自己总结的 :-)</p>

<ol>
<li>提出创新时代的四个问题：速度、稳定、伸缩、移动</li>
<li>云架构在这四个问题上大致有何思路。</li>
<li>针对四个问题，云架构有五个措施：十二因子、微服务、自助基础设施、基于API协作、反脆弱</li>
</ol>


<p>（<a href="/blog/2015/05/29/cloud3/">后文继续，需要转变</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>monolithic application architectures<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
</feed>
