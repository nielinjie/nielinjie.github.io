<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[聂同学]]></title>
  <link href="http://nielinjie.github.io/atom.xml" rel="self"/>
  <link href="http://nielinjie.github.io/"/>
  <updated>2015-10-04T15:03:48+08:00</updated>
  <id>http://nielinjie.github.io/</id>
  <author>
    <name><![CDATA[nielinjie]]></name>
    <email><![CDATA[nielinjie@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[技术栈是架构设计的目的么？]]></title>
    <link href="http://nielinjie.github.io/blog/2015/10/02/techstack/"/>
    <updated>2015-10-02T22:46:38+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/10/02/techstack</id>
    <content type="html"><![CDATA[<p>不是。</p>

<p>相比于架构设计的目的，技术栈更像是架构执行的手段。</p>

<p>“为了实现那样的架构设计，我们将使用这些的技术。……”</p>

<!--more-->


<p> <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup><br/>
 <sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>另一个角度说来，技术栈也确实常作为架构设计的重要产出物，体现着设计结论的一些方面。所以类似“XX技术在XX公司的应用”这样的句型，作为架构分享文章的标题经常出现。读到这样的文章，希望同学们以正确的姿势打开，从技术栈的展示中，窥见架构设计的来龙去脉。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>既然架构设计和执行是连贯不分的一个整体，那么区分前者的输出和后置的输入有什么意义呢？我认为是有意义的。这是孰本孰末，孰因孰果的问题。试想这样的情况：架构执行过程中，如果发现栈中某技术不适用，是否可以更改？如果是执行的手段，我们可以便宜更改，只要不偏离设计结论；如果是设计的目的，我们不能随意更改，因为要改结论需要从论点、论据、论证一一检视更改。<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“别让牛人写代码”]]></title>
    <link href="http://nielinjie.github.io/blog/2015/09/27/nocoolcode/"/>
    <updated>2015-09-27T16:51:48+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/09/27/nocoolcode</id>
    <content type="html"><![CDATA[<p>同意。</p>

<!--more-->


<p>牛人写的代码固然威力无穷，奈何太贵。不仅现在贵，将来更贵。</p>

<p>一般人看不懂，改不动，又不敢丢。活活贵死整个团队。</p>

<p>如果有别的办法，不要依赖于牛人写代码。</p>

<p><img src="http://nielinjie.github.io/images/nocoolcode/nocoolcode.jpg" width="350"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“伪代码是好代码”]]></title>
    <link href="http://nielinjie.github.io/blog/2015/09/25/fakecode/"/>
    <updated>2015-09-25T20:51:27+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/09/25/fakecode</id>
    <content type="html"><![CDATA[<p>可以这么说。</p>

<!-- more -->


<p>通常伪代码都直接描述了某个层面的业务逻辑，是比较典型的领域特定语言（DSL）。同时，也往往符合DDD中对统一语言的期望。</p>

<p>在系统中，如果有一个层次的代码可以集中精力于表达业务逻辑，而不用操心各种质量约束，无疑是理想的。</p>

<p>伪代码确实是好代码，如果它能如愿运行的话。</p>

<p><img src="http://nielinjie.github.io/images/fakecode/fakecode.jpg"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[以场景思维辅助风险驱动架构设计]]></title>
    <link href="http://nielinjie.github.io/blog/2015/08/13/scenario/"/>
    <updated>2015-08-13T15:15:34+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/08/13/scenario</id>
    <content type="html"><![CDATA[<p>“风险驱动架构设计”<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>需要架构师和团队及时发现风险。这一点是依赖团队经验的。这样的依赖常常引发人们的焦虑：是否所有的风险都被及时发现进而处理了？</p>

<p>不用说缺乏相关经验的团队，即使是经验丰富的团队，随着进入陌生的业务领域或技术环境，也不能避免再次陷入到这样的焦虑之中。</p>

<!-- more -->


<p>如何缓解焦虑，建立大家的信心呢？我们做了一些尝试——</p>

<p>每当我们害怕遗漏的时候，我们有个朴素的方法：列出所有可能，一一检视，从中挑出那些需要处理的。</p>

<p>怎么才能列出“所有”可能的工程风险呢？我们尝试的是场景思维：我们以软件工程中常见的场景为线索——</p>

<ul>
<li>系统处理典型需求的场景</li>
<li>发布新功能的场景</li>
<li>业务量发生显著变化的场景</li>
<li>常见故障和应对的场景</li>
</ul>


<p>根据业务领域、技术环境以及团队结构等因素的不同，这些场景中会发生不同的事件。通过虚拟这些场景和其中的事件，一一检视这些事件中可能蕴含的风险，就可以使我们发现风险的工作系统化。</p>

<p>有同学问到：去寻求一个“完整的”风险列表，不是跟“恰如其分的、不完备的”原则矛盾了么？并不是这样的。我们说的要恰如其分和不完备，指的是架构活动不追求全面，而不是说考虑范围不考虑全面。通过场景思维剪除掉不必要应对的风险，以后的架构活动仍然是恰如其分的。</p>

<p>我们可以从另一个角度来概述这件事情——</p>

<p>“不能发现所有风险”这个风险，是我们可能需要应对的第一个风险。如果需要应对，那我们的措施就是“场景思维法”。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://nielinjie.github.io/blog/2014/03/31/aa1/">“风险驱动架构设计”</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[雪人（五）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/08/04/said-xueren4/"/>
    <updated>2015-08-04T21:10:16+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/08/04/said-xueren4</id>
    <content type="html"><![CDATA[<p>“最近好多展览，这些雪人真是太漂亮了！”<br/>
“是的。可惜不能看到建造和维护这些雪人的展览。”</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构工作中的三个关键词]]></title>
    <link href="http://nielinjie.github.io/blog/2015/07/29/3words/"/>
    <updated>2015-07-29T10:21:30+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/07/29/3words</id>
    <content type="html"><![CDATA[<p>这里是我在架构工作中强调的三个关键词。是我工作桌面上保留时间最长的一个Post，也许会一直保留下去吧。</p>

<!-- more -->


<h4>Structure</h4>

<p>结构。在有些观点中，结构就是架构的定义，比如：“架构就是系统中的元素以及它们之间的关系”。<br/>
架构的结果最终被体现为系统中的结构，架构的价值，也是通过系统中的结构体现。</p>

<h4>Risk</h4>

<p>风险。解决风险是架构的目的。所有的架构活动，都是为了消除或者减轻工程风险。<br/>
把风险作为架构过程的驱动力，是进行“恰如其分”的架构过程的有效选项。</p>

<h4>View</h4>

<p>视野。架构师需要随时与开发团队共享视野。<br/>
一方面保证开发团队对架构的认识和认同，另一方面保证架构师对需求的了解。</p>

<h4>最后</h4>

<p>来个靓照。是的，它有些沧桑了。:-D</p>

<p><img src="http://nielinjie.github.io/images/3words/post.jpg" width="300"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[中文字体]]></title>
    <link href="http://nielinjie.github.io/blog/2015/07/28/typo/"/>
    <updated>2015-07-28T16:41:56+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/07/28/typo</id>
    <content type="html"><![CDATA[<p><a href="http://typo.sofi.sh">typo.css</a> 提供的中文字体不错——</p>

<p><code>
PingFang SC, Lantinghei SC, Microsoft Yahei, Hiragino Sans GB, Microsoft Sans Serif, WenQuanYi Micro Hei, sans;
</code></p>

<!--more-->


<p> <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>2015年9月30日升级OSX到EI Capitan后，PingFang SC变难看了，改为了PingFang SC Light。话说只有我一个人觉得PingFang SC不好看么？只有Light和Thin勉强能看。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[问架构师候选人什么问题？]]></title>
    <link href="http://nielinjie.github.io/blog/2015/07/25/questions/"/>
    <updated>2015-07-25T09:57:11+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/07/25/questions</id>
    <content type="html"><![CDATA[<p>面试时交流时间有限，应该问你的架构师候选人什么问题？</p>

<!--more-->


<h4>Q1：架构工作有些什么步骤？架构工作与其它开发工作的关系如何？……</h4>

<p>有些架构师认为此类问题比较重要。只有弄清楚了这些问题，才能算是掌握了架构方法论，能够保证可控可重复地进行架构设计。而另一些架构师则认为，只要能产出切实可用的系统，架构设计是什么，它如何达成的都不重要。</p>

<p>个人认为方法论是重要的。你团队中的架构过程和成果需要积累和传承，适当的方法论可以在架构师（包括开发团队）之间统一讨论语言和产出物，利于积累和传承。</p>

<p>特别是，如果你认为你的架构师应该负责建立或提高架构和开发的团队和流程，你需要问此类问题。</p>

<h4>Q2：假设现在需要搭建XX类型的系统，你如何思考？</h4>

<p>这类问题是开放式的主观题。涵盖范围很广，可以全面地考察架构师的思维方式、方法论、知识、经验，所以被广泛使用。这类问题很好用，但需要注意。它常常引来候选人的长篇大论，从方法到模式，从前台到后台，从需求到设计，…… 所以交流过程中需要控制和引导，避免被淹没在回答当中，当候选人提到了想要重点考察的领域，需要有针对性地进行专门的提问和讨论。</p>

<h4>Q3：假设现在遇到X问题，解决方案是什么？</h4>

<p>这类问题考察的是架构师的经验。如果他<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>遇到和解决过同类问题，并且记住了解决方案，他就能回答，如果没有，就不能回答。因为没有人能够在四目相对的几秒十几秒时间里解决一个陌生的架构问题。要知道架构决策和设计不是一个靠急智能够胜任的工作。</p>

<p>所以如果你需要架构师招聘进来以后能够立即解决一个问题——也就是你提问中的那个问题，你需要问这类问题。个人认为这种情况要尽量避免，因为这样就放弃了对其它方面素质的考量和选择。</p>

<h4>Q4：AA设计模式中……、BB数据库中……、CC算法中……、DD类里面……</h4>

<p>这类是涉及到某些领域的细节问题，如果你需要架构师兼任此领域的技术专家（这种情况常见），那你需要问这类问题。</p>

<p>有些架构师可能对某些领域的细节问题解决能力不足，但他隔离和描述问题的能力很强，进而可以很快从外部找到并应用现成的解决方案。这样的架构师虽然称不上技术专家，但一定程度上其实可以解决大量的具体问题。</p>

<p>依据你面对的问题、团队规模及可以负担的成本，你需要判断是否需要真正的技术专家。</p>

<h4>Q5：在你做过的YY事情中，是如何考虑ZZ问题的？</h4>

<p>这类问题跟Q2问题功能相似。同时它可以避免Q2问题的一个弱点：万一候选人没有XX类型系统的工作经验，考察就没办法进行下去。除非：你的职位必需这类系统的知识和经验。这种情况常见，比如招聘互联网产品的架构师往往认为必需“大并发系统”的知识和经验。</p>

<h4>Q6：最近在干什么？学什么？比较关注什么？</h4>

<p>个人比较喜欢此类问题。但这类问题完全没有参考答案，需要提问人注意从回答中挖掘想要的信息。比如候选人对职业生涯的规划、对技术走势的判断等等。否则问答完了之后，对候选人是否适合职位的判断帮助不大。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>文中所有的“他”，均不特指男性。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[单体优先还是微服务？]]></title>
    <link href="http://nielinjie.github.io/blog/2015/06/28/monolithf/"/>
    <updated>2015-06-28T16:58:26+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/06/28/monolithf</id>
    <content type="html"><![CDATA[<p>单体优先还是直接采用微服务？这个问题随着马丁大叔的文章Monolith First<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>发布，显得再次热闹起来。</p>

<p>在我看来，从三个方面尝试分析这个问题。</p>

<ol>
<li>微服务架构和单体架构区别是什么？</li>
<li>系统建立之初这些区别意味着什么？</li>
<li>如果系统建立之初使用单体架构，后续过渡到微服务架构代价如何？</li>
</ol>


<!-- more -->


<h3>微服务架构和单体架构区别是什么？</h3>

<p>微服务架构与单体架构的区别，本质是系统各部件间分隔的强度大小。</p>

<p>从下面几个方面看一看：</p>

<table>
<thead>
<tr>
<th><i></i> </th>
<th>微服务 </th>
<th> 单体</th>
</tr>
</thead>
<tbody>
<tr>
<td>领域分隔 </td>
<td> 领域被分隔为微服务。分隔力度大，相互间的影响较小。微服务可以各自拥有不同的进化节奏，不同领域的创新可以分别实施、快速落地。 <br/> 领域间的调用相对困难，需要一些基础服务帮助，比如服务注册和寻址等。 </td>
<td> 领域的分隔表现为模块的分隔，其间的联系简单直接。</td>
</tr>
<tr>
<td>团队分隔    </td>
<td> 团队按微服务配置。成员专注于小的领域和代码集。沟通成本低。容易学习。<br/> 需要部件之间紧密协作时相对困难，比如当代码需要在部件之间移动。  </td>
<td> 整个系统一个团队。如果系统变得庞大，成员就需要学习大量的代码和领域知识，团队内的沟通和协作也变得低效。不得不分割团队时容易按职责分割，形成竖井团队<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。</td>
</tr>
<tr>
<td>技术分隔    </td>
<td> 在不同的微服务中，可以根据不同的业务特性分别选择适当的技术。包括可以分别选择适当的存储策略。    </td>
<td> 整个系统（甚至整个企业）统一的技术栈，管理起来看似简单。但有时候统一的标准并不适合所有的实际情况。</td>
</tr>
<tr>
<td>运行时分隔 </td>
<td> 各部件通常运行于不同的进程。容易进行错误隔离。可以分别伸缩。<br/> 运行时需要管理的单位较多，相对困难，需要一些专门的运营工具。</td>
<td> 通常运行于同一个进程。部件间协作的额外开销很小。</td>
</tr>
</tbody>
</table>


<h3>系统建立之初这些区别意味着什么？</h3>

<p>通过上面的罗列比较我们可以看到：对于复杂系统，微服务架构可以有效地分隔复杂度。
但微服务架构有风险：首先需要前期就对领域有良好的认识以便分割。其次需要一定的基础服务和工具。如果团队并不熟悉这种相对较新的架构，学习和适应的成本还是比较高的。
如果我们的系统在建立之初比较简单，在各个方面基本上并不需要高强度的分隔，单体架构往往就能够满足要求。</p>

<p>我们看看什么情况下可能有可能直接从微服务架构开始：</p>

<ul>
<li>我们的系统所面对的领域规模很大，需要进行分割；同时，我们很清楚如何分隔。（……，好吧，这种情况基本没有，囧）</li>
<li>我们的团队规模太小，从一开始就无法单独承担系统的规模。</li>
<li>我的企业默认架构就是微服务，很多系统已经实践过了。</li>
<li>我的老板认为微服务很酷，必须上。</li>
<li>……</li>
</ul>


<p>这些情况下，如果各方充分认识到微服务的代价并作出应对预案，是可以直接应用微服务架构的。</p>

<p>在所有的代价中，有一种最重要，值得再说一遍：领域划分不清晰的情况下请务必慎重，在微服务间移动领域逻辑是非常昂贵的。</p>

<h3>已有单体架构系统过渡到微服务架构代价如何？</h3>

<p>马丁大叔提出的“扼死大法”<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>是一种自然有效的过渡方式。但跟其他所有的方式一样。
这个办法的难度和相关代价还是取决于单体本身的结构特点。
如果单体自身拥有良好的结构，容易从中剥离出相对独立的领域逻辑。那我们可以有条不紊逐步剥离：</p>

<ol>
<li>为新特性创建微服务，单体保持不变。</li>
<li>在单体中识别内聚的子领域，对应地各自剥离为微服务。</li>
<li>按照业务价值和变化频度安排优先级。</li>
<li>并不追求完全消灭单体。</li>
</ol>


<p>另一种情况，单体本身是一个大泥球。那就没有那么幸运了，我们必须先整理单体本身。</p>

<h3>结论</h3>

<ul>
<li><p>单体优先，同时请做好准备，你可能很快需要过渡到微服务。所以做一个“微服务友好”的单体，并适时开始基础服务和团队技能的准备。</p></li>
<li><p>读到这里仍然觉得自己应该立即微服务的同学：请不犹豫地微服务吧。</p></li>
</ul>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://martinfowler.com/bliki/MonolithFirst.html">Monolith First</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:1">
<p>“竖井团队”被认为在大部分情况下是反模式。参见<a href="http://www.thoughtworks.com/radar/techniques/inverse-conway-maneuver">“反Conway策略”</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><a href="http://www.martinfowler.com/bliki/StranglerApplication.html">StranglerApplication</a><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（六）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/06/07/cloud6/"/>
    <updated>2015-06-07T11:15:31+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/06/07/cloud6</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/06/07/cloud5/">接前文</a>）</p>

<h2>分布式系统指南</h2>

<p>当我们开始构建由微服务组成的分布式系统，我们需要应对单体架构系统一般不会需要的非功能需求。有时候需要跟物理定律周旋，比如一致性、延迟、网络分割等。然而另一些问题比如脆弱和可管理性就可以用相对通用的模式来解决。下面我们将介绍这方面的一些实践。</p>

<!--more-->


<p>这些实践来自于<a href="http://projects.spring.io/spring-cloud/">Spring Cloud</a> 和<a href="http://netflix.github.io">Netflix OSS</a> 系列项目的组合使用。</p>

<h3>有版本的分布式配置</h3>

<p>我们已经讨论了<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>适当的配置管理机制的重要性，提到过配置通过操作系统级别的环境变量注入。这种方式非常适合简单系统。但当系统复杂性增加，我们可能需要更多的配置功能，比如：</p>

<ul>
<li>为正在运行的应用改变日志级别，以便诊断生产问题。</li>
<li>改变接收消息的线程数。</li>
<li>报告所有的配置更改，支持生产系统的监管审计。</li>
<li>为正在运行的应用开关某个功能。</li>
<li>支持配置中的保密内容，比如密码。</li>
</ul>


<p>为了支持这些能力，我们的配置机制需要有如下的特性：</p>

<ul>
<li>有版本</li>
<li>可以审计</li>
<li>加密</li>
<li>刷新不需重启</li>
</ul>


<p>Spring Cloud项目中有个<a href="http://cloud.spring.io/spring-cloud-config/">配置服务器</a>支持这些特性。这个配置服务器保存了应用的配置文件，后台是一个git仓库，提供一套REST API（图3-1）。</p>

<p><img src="http://nielinjie.github.io/images/cloud/springConfig.png" title="[图3-1. The Spring Cloud Config Server]" ></p>

<p>图3-1. The Spring Cloud Config Server</p>

<p>剩下的问题是如何可以不重启应用客户端修改配置。这个能力由Spring Cloud的另一个模块——<a href="http://cloud.spring.io/spring-cloud-bus/">总线</a>提供。这个模块用一个轻量级的消息中间件连接分布式系统中的各节点。它可以用来广播状态变化，比如我们的配置更改（图3-2）。
只要简单地向连入总线的任何应用的<code>/bus/refresh</code>地址发送一个HTTP POST，我们就可以提示所有连入的应用更新他们的配置值，通常更新到配置服务器上的最新值。</p>

<p><img src="http://nielinjie.github.io/images/cloud/springBus.png" title="[图3-2. The Spring Cloud Bus]" ></p>

<p>图3-2. The Spring Cloud Bus</p>

<h3>服务注册和发现</h3>

<p>当我们建立分布式系统，我们的代码和它的依赖之间就必须通过网络沟通。我们如何有效地将我们的微服务联系起来呢？</p>

<p>云中的一种常见的架构模式（图3-3）是建立前端（应用）服务和后端（业务）服务，后端服务通常不会被直接访问，而是通过前端服务间接访问。服务注册表存放了所有服务的信息，前端服务中有一个客户端库<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>可以到这些信息，从而可以处理路由和负载均衡事务。</p>

<p><img src="http://nielinjie.github.io/images/cloud/serviceRegistration.png" title="[图3-3. Service registration and discovery]" ></p>

<p>图3-3. Service registration and discovery</p>

<p>我们使用各种<a href="https://en.wikipedia.org/wiki/Service_locator_pattern">Service Locator</a>和<a href="https://en.wikipedia.org/wiki/Dependency_injection">Dependency Injection</a>解决这个问题，面向服务架构长期使用各种服务注册机制。这里我们使用一个类似的方案：<a href="https://github.com/Netflix/eureka">Eureka</a>——来自于Netflix OSS项目，可以在服务定位的同时处理中间层服务的负载均衡、failover机制。<a href="http://cloud.spring.io/spring-cloud-netflix/">Spring Cloud Netflix</a>项目进一步简化了对Eureka的使用，它提供了一个基于Annotation的配置模型。</p>

<p>所有使用<a href="http://projects.spring.io/spring-boot/">Spring Boot</a>的应用都可以通过简单添加<code>@EnableDiscoveryClient</code>来获得服务注册和发现功能。</p>

<h3><a name="rlb"></a>路由和负载均衡</h3>

<p>简单的循环负载均衡在很多场景下是很有效的。但在云环境下的分布式系统需要进一步的路由与负载均衡行为。以前这通常由外部的集中的负载均衡服务提供。然而这种服务往往没有足够的信息和上下文，它们也没办法为应用提供最佳选择。同时，集中的解决方案存在单点失效的问题，当他们出问题了整个架构都要受到影响。</p>

<p>云架构把路由和负载均衡的职责转移到客户端。这种方案的一个例子是来自Netflix OSS项目的<a href="https://github.com/Netflix/ribbon">Ribbon</a>（图3-4）。</p>

<p><img src="http://nielinjie.github.io/images/cloud/clientLoadBalancer.png" title="[图3-4. Ribbon client-side load balancer]" ></p>

<p>图3-4. Ribbon client-side load balancer</p>

<p>Ribbon提供了丰富的功能：</p>

<ul>
<li>内建多种负载均衡规则

<ul>
<li>循环</li>
<li>平均响应时间加权的循环</li>
<li>随机</li>
<li>可用性过滤（避免tripped circuits和大并发连接数）</li>
</ul>
</li>
<li>定制均衡规则插件</li>
<li>与服务发现方案（包括Eureka）的可拔插集成</li>
<li>Cloud-native intelligence such as zone affinity and unhealthy zone avoidance</li>
<li>内建错误容忍</li>
</ul>


<p>就跟Eureka类似，Spring Cloud Netflix 项目进一步简化了对Ribbon的使用，将注入<code>DiscoveryClient</code>变为注入<code>LoadBalancerClient</code>，就可以从直接使用Eureka切换为使用Ribbon</p>

<h3><a name="ft"></a>错误容忍</h3>

<p>分布式系统潜在的错误比单体系统要多。现在每一个请求都需要使用数十个甚至上百个不同的微服务，其中的一个或多个出问题几乎是肯定的。</p>

<blockquote><p>如果不采取必要地错误容忍措施，30个依赖服务的系统每个月将有两个小时多的宕机时间，即使每个服务可用性都是99.99%。（99.99%^30 = 99.7% uptime = 2+ hours in a month）—<em>Ben Christensen, Netflix Engineer</em></p></blockquote>

<p>我们如何防止类似的错误堆积呢？</p>

<p>Mike Nygard论述<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>了几种有用的模式：</p>

<ul>
<li><a name="bc"></a>断流器<br/>
断流器会隔离一个服务，如果发现它的依赖服务状态不佳的话。断流器通常被实现为一个状态机（图3-5）。当它处于闭合状态，调用就简单地被传递到依赖。如果调用失败了，断流器开始对失败计数，当在特定时间内失败次数达到了特定值，断流器就切换到断开状态。当断流器处于断开状态，任何调用都会立即返回，根本不会真正尝试调用依赖。再过了一个特地时间过后，断流器切换到半开状态。在半开状态，调用会被传递到依赖，如果成功，断流器切换到闭合状态，否则切到断开状态。</li>
</ul>


<p><img src="http://nielinjie.github.io/images/cloud/stateMachine.png" title="[图3-5. A circuit breaker state machine]" ></p>

<p>图3-5. A circuit breaker state machine</p>

<ul>
<li>隔板<br/>
隔板分割服务，防止整个服务因为局部错误而整体失效。软件系统可以从多个层面应用隔板。简单地分割为微服务就是第一种应用。将应用进程分割为Linux容器<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>，使得单个进程不会影响整个机器，也是一种应用。还有个例子是将并行运算分散到多个线程池中。</li>
</ul>


<p>Netflix在库<a href="https://github.com/Netflix/Hystrix">Hystrix</a>中加入了很强大错误容忍功能。Hystrix通过<code>HystrixCommand</code>来将代码包装在断流器中。</p>

<p>Spring Cloud Netflix项目通过<code>@EnableCircuitBreaker</code>注解，在Spring Boot应用中加入Hystrix部件。并且借助一系列<a href="https://github.com/Netflix/Hystrix/tree/master/hystrix-contrib/hystrix-javanica">捐献的注解</a>使得编程更加的简单。</p>

<p>Hystrix不同于其他的断流器，它还应用隔板模式，将各个断流器封装在各自的线程池中。它还收集一些有用的数据：</p>

<ul>
<li>流量</li>
<li>请求速率</li>
<li>错误占比</li>
<li>Hosts reporting</li>
<li>延迟分布</li>
<li>请求结果：成功、失败、拒绝</li>
</ul>


<p>这些数据作为事件流散发，可以用另一个Netflix OSS工具—— <a href="https://github.com/Netflix/Turbine">Turbine</a>进行综合。单独的综合的数据都可以通过一个Hystrix面板展示出来（图3-6），面板对分布式系统的健康状况提供了很好地可视化。</p>

<p><img src="http://nielinjie.github.io/images/cloud/dashboard.png" title="[图3-6. Hystrix Dashboard showing three sets of circuit breaker metrics]" ></p>

<p>图3-6. Hystrix Dashboard showing three sets of circuit breaker metrics</p>

<h3><a name="api"></a>API网关和边缘服务</h3>

<p>我们已经提到了<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>微服务中服务方的服务整合，我们来具体看看它的必要性：</p>

<ul>
<li>延迟<br/>
移动端通常使用比较慢速的网络，如果应用需要依次跟几十几百个服务通信，那延迟是很难接受的。容易理解并行发起请求是很有必要的。相比于在各个不同的移动端平台上实现并行模式，在服务端实现会比较便宜也不容易出错。</li>
<li>续航<br/>
就算网络速度不是问题，客户端跟大量微服务打交道还是会有问题。使用网络对于移动端来说是很消耗电池电量的。移动开发者通常都希望减少与服务端的通信来增强用户体验。</li>
<li>设备多样性<br/>
移动端的设备多样性非常明显。比如厂商、尺寸、操作系统、编程语言等等都有很多不同。</li>
</ul>


<p><a href="http://microservices.io/patterns/apigateway.html">API网关</a>模式就是将移动端开发的负担转移到服务端。API网关就是一个普通的微服务，只不过它是与单个移动应用对应的，为它提供单一的后台入口。它每个请求都会跟几十几百个服务并行通信，将所有返回结果综合起来再返回到移动端。如果有必要，它也处理协议翻译的工作，比如HTTP翻译为AMQP。</p>

<p><img src="http://nielinjie.github.io/images/cloud/apiGateway.png" title="[Figure 3-7. The API Gateway pattern]" ></p>

<p>Figure 3-7. The API Gateway pattern</p>

<p>API网关可以用任何语言、运行时或者框架实现，只要它能支持web编程、并发以及跟后台服务通信的协议。流行的选择包括Nodejs（有reactive编程模型）和Go（有简单的并发模型）。</p>

<p>如果使用Java，可以考虑<a href="https://github.com/ReactiveX/RxJava">RxJava</a>， <a href="http://reactivex.io">Reactive Extensions</a> 的Java实现。毕竟如果只使用Java提供的原生特性，合并并行处理的结果这一点很难做好。</p>

<h2>总结</h2>

<p>以下又是译者自己总结的 :&ndash;)</p>

<ul>
<li>分解： 新特性作为微服务、防腐层、扼杀单体</li>
<li>分布式系统： 配置服务和管理总线、动态服务发现、去中心化的负载均衡、断流器和隔板、API网关</li>
</ul>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/#12f">十二因子应用</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><a href="#rlb">路由和负载均衡</a><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>书：<em>Release It!</em><a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p><a href="http://nielinjie.github.io/blog/2015/06/02/cloud4/#c">容器化</a><a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p><a href="http://nielinjie.github.io/blog/2015/05/23/cloud/#mc">移动应用和客户端多样性</a><a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（五）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/06/07/cloud5/"/>
    <updated>2015-06-07T11:15:31+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/06/07/cloud5</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/06/02/cloud4/">接前文</a>）</p>

<h1>迁移指南</h1>

<p>现在我们已经定义了云架构，简短讨论了企业如果要使用云架构需要些什么转变。现在我们谈谈具体的技术。这里更多的是提供简短介绍和进一步阅读的链接。</p>

<!--more-->


<h2>分解指南</h2>

<p>我们已经讨论了分解数据、服务、团队，那么，我们怎么才能实现这些呢？好问题，我们如何打破现存的巨大的一切，前进到云架构呢？</p>

<p>我已经看到一些公司成功地进行了渐进式迁移，有一些可以借鉴的模式。这里有一些公开的例子：<a href="https://blog.yourkarma.com/building-microservices-at-karma">Karma</a>、<a href="https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith">SoundCloud</a>。</p>

<p>下面我们将一步一步看看如何将单体上的服务迁移到云。</p>

<h3>新特性作为微服务</h3>

<p>有点意外的是，第一步并不是打碎单体。我们现在假设在单体上你有一些待实现的特性。事实上，如果你的单体根本没打算实现新功能，那恐怕根本没有必要去分解它。我们首要的目的是快速变化，如果你的系统根本没打算要变化，谈何快速变化呢？</p>

<blockquote><p>团队认为改变架构的好办法并不是立即分拆单体，而是不再在上面增加任何新东西，新东西都建立为微服务——<em>Phil Calcado, SoundCloud</em></p></blockquote>

<p>这个策略显然的好处就是我们建立新的微服务的难度很低，毕竟从头开始建立要比从一个<a href="http://www.laputan.org/mud/">大泥球</a>上动刀要容易得多。</p>

<p>当然不可避免的，我们的微服务需要跟单体沟通，我们如何处理这个问题呢？</p>

<h3>反腐层</h3>

<blockquote><p>因为我们的逻辑还在Rails写的单体中，所有的微服务都还得这样或那样地跟它沟通。——<em>Phil Calcado, SoundCloud</em></p></blockquote>

<p>DDD讨论了<em>反腐层</em>，它的目的是让两个不同领域模型的系统可以相互沟通，而不必让一个系统的领域模型同化另一个。当你在新的微服务中实现新的功能的时候，你肯定不希望新功能耦合在既有单体的内部业务知识上。反腐层是一种建立API合同的办法，让单体看起来就像是另一个微服务。</p>

<p>Evans 将反腐层的实现划分为三个子层：</p>

<ul>
<li>Facade —— 在这里Facade的目的是简单地集成单体的接口，因为很可能单体的接口并不是按照现行集成原则来设计的。重要的是，这里不要改变单体的领域模型，不要把集成和翻译耦合在一起了。</li>
<li>Adapter —— Adapter中我们定义我们需要的“服务”。Adapter知道如何从我们的系统接收一个请求，然后将请求转发给单体。</li>
<li>Translator —— Translator的任务就是为我们的服务和单体之间的请求和响应转换领域模型。</li>
</ul>


<p>这三个松耦合的部件解决三个问题：</p>

<ul>
<li>系统集成</li>
<li>协议翻译</li>
<li>模型翻译</li>
</ul>


<p>现在剩下的问题是在哪一层进行通信。DDD中Evans讨论了两种办法：第一种，<em>facade to system</em>，当你无法改变遗留系统的时候使用。在这里我们假定我们是可以修改遗留单体的，那么我们主要讨论第二种，<em>adapter to facade</em>，也就是我们将Facade放进到遗留单体中，所以通信在Adapter和Facade之间进行。因为显然两个专门现写的部件通信起来容易些。</p>

<p>最后值得注意的是，防腐层也能促进双向沟通，毕竟遗留单体有时候也是需要跟新建立的微服务沟通的。</p>

<h3>扼杀单体</h3>

<blockquote><p>随着架构变化的进行，团队得以在一个灵活得多的环境里构建新特性和改进，剩下的问题是，我们如何从单体中提取已有特性？——<em>Phil Calcado, SoundCloud</em></p></blockquote>

<p>我从Martin Fowler的文章<a href="http://www.martinfowler.com/bliki/StranglerApplication.html">StranglerApplication</a>中借用了“扼杀单体”的说法。在这篇文章中Fowler介绍：在老系统的周边逐渐建立一个新系统，让老系统生长缓慢，几年后最终被扼杀。我们要做的正是如此：通过一系列的微服务和反腐层，我们在老系统周边逐渐建立新的云中系统。</p>

<p>两个原则帮助我们选择要提取的特性：</p>

<ul>
<li>SoundCloud制订了第一个原则：在单体中识别界限上下文。正如我们前面讨论的，界限上下文要求内部一致的领域模型。遗留单体的领域模型显然不会是内部一致的，现在我们就开始从中识别内部一致的子模型。这些就是我们提取的候选。</li>
<li>第二个原则讨论了优先级：哪些候选先提取？我们可以通过回顾迁移到云的意义来回答这个问题：快速创新。哪个特性提取后最有助于快速创新？我们显然选择那些些现有业务需要它快速变化的特性。</li>
</ul>


<h3>怎么才算完成？</h3>

<p>我们怎么知道我们完成了？一般有两个完成标志：</p>

<ul>
<li>单体完全被扼杀了。所有的界限上下文都被提取成了微服务。最后一步就是识别出并干掉那些不再需要的防腐层。</li>
<li>单体被扼杀到了一定程度：要继续提取特征的话，成本已经超过了收益。有时候单体的一部分很稳定，它们几年都不变，工作得也很好。那么提取这些部分就没有什么价值，如果同时维护相关的反腐层成本也够低，那我们就让它们长期保留好了。</li>
</ul>


<p>（<a href="http://nielinjie.github.io/blog/2015/06/07/cloud6/">后文继续，分布式系统指南</a>）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（四）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/06/02/cloud4/"/>
    <updated>2015-06-02T11:15:31+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/06/02/cloud4</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/05/29/cloud3/">接前文</a>）</p>

<h2>组织转变</h2>

<p>这一节我们将讨论为了更好地应用云架构，组织在建立团队方面将如何改进。这个理论的背后是著名的<em>Conway’s Law</em>，我们的办法是围绕一个长期的产品线建立多职责的团队，而不是鼓励每种职责的人员呆在自己单独的团队里（比如testing）。</p>

<!--more-->


<h3><a name="bct"></a>业务能力<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>团队</h3>

<blockquote><p>任何组织设计的产品，它的设计结构都跟组织的交流结构一致。—— Melvyn Conway</p></blockquote>

<p>我们已经讨论了把IT划分为竖井的做法。自然地，这时我们也把每个人放进了这样的竖井里。那么我们来看看需要开发一个新软件时会发生什么。</p>

<p>通常的做法是建立一个项目团队，指定一名项目经理。然后项目经理会跟各个竖井打交道，获得项目所需的各种人员。从上面引用的<em>Conway’s Law</em>我们会看到，这种团队自然就产出了类似竖井结构的架构：</p>

<ul>
<li>数据操作层</li>
<li>服务层</li>
<li>Web MVC 层</li>
<li>消息层</li>
<li>……</li>
</ul>


<p>这些层次横亘在各个业务能力间，各个业务能力要想不影响其他地创新和落地，变得比较困难。</p>

<p>很多公司想要迁移到云架构，比如按业务能力划分微服务。他们应用的是Thoughtworks所谓的
<a href="http://www.thoughtworks.com/radar/techniques/inverse-conway-maneuver">“反Conway策略”</a>。不是按组织结构来决定架构，而是反过来，按照想要的架构来重新调整组织架构。按照Conway的观点，只有这样，你想要的架构才会最终出现。</p>

<p>所以，作为迁移到DevOps文化的一部分，团队跨职责并按照业务能力来组织，他们开发产品而不是项目。产品是长期存在的，直到他们不再具有业务价值。交付一个业务能力需要的人员都在一个团队中——开发、测试、发布、运营，代码不需要在各个团队间传来传去。这种团队往往也叫“<a href="http://www.fastcompany.com/50106/inside-mind-jeff-bezos">两个披萨团队</a>”，也就是说如果两个披萨不够这个团队分，那这个团队就太大了点。</p>

<p>现在我们来看如何决定要建立什么团队。如果我们遵从“反Conway策略”，我们将从组织的领域模型开始，找到能够封装在“界限上下文”<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>里地业务能力。一旦确定了这些业务能力，我们就建立相关的业务能力团队，负责这个业务能力的整个生命周期，同时也负责从开发到运营的整个循环。</p>

<h3><a name="pt"></a>平台团队</h3>

<p>业务能力团队依赖于“敏捷的自助基础设施”<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>。实际上，我们可以定义一个业务能力，叫做“可以开发、部署、运营业务能力的业务能力”。这个能力由平台团队负责。</p>

<p>平台团队根据业务能力团队的要求运营着“敏捷的自助基础设施”。如果公司自己运行云平台，那平台团队需要负责数据中心和了解硬件。</p>

<p>IT运营往往通过各种ticket系统与客户交互。但平台团队运营的是“自助”的平台，它要以不同的方式交互。就像业务能力团队通过API合同跟其他团队协作，平台团队也为平台定义一套合同。业务能力团队不是将环境和数据请求放到等待队列等待实施，而是请求并获得一个自动发布管道，需要的时候能够自行处理环境和数据。</p>

<h2>技术转变</h2>

<p>现在我们来看看迁移到云中的DevOps可能遇到的实现问题。</p>

<h3>分解单体<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>架构</h3>

<p>传统的n层单体架构部署到云基础设施很难运行得好。因为它们往往就运行环境做了一些假设，然而云基础设施并不支持，比如：</p>

<ul>
<li>mount好的共享文件系统</li>
<li>P2p的应用服务集群</li>
<li>共享运行库</li>
<li>已知位置的配置文件</li>
</ul>


<p>这些假设都基于这种架构的应用被部署在长期存在的基础设施上，它们跟云基础设施弹性短期的想法并不兼容。如果应用并没有这些环境假设呢？还是有问题：</p>

<ul>
<li>单体架构将所有业务能力的变化绑在一起，不利于各个业务能力的创新分别落地。妨害了创新的快速。</li>
<li>单体架构中的服务很难单独伸缩，负载效率难以提升。</li>
<li>新加入组织的员工难以适应，他们需要学习整个领域和大量的代码，没有几个月的时间，他们很难成为真正有效率的开发人员。</li>
<li>开发组织通过增加人员来扩大规模很难，增加沟通和协作的成本。</li>
<li>技术栈长期不变，引入新技术风险较大。</li>
</ul>


<p>以上清单正好是“微服务”<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>的优势清单逐条反过来（<s>我Kao</s>）。同时，将组织划分为业务能力团队，也要求将单体架构分解为微服务。只有这样，才能充分享有我们迁移到云基础设施的好处。</p>

<h3><a name="dd"></a>分解数据</h3>

<p>分解单体架构并不足够。数据模型也需要分解。如果业务能力团队看似自治但却被限制在同一个数据上协作，单体架构对创新的阻碍只不过搬到了数据层面而已。</p>

<p>DDD认为<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>我们的成功很大程度上依赖于我们的领域模型的质量。领域模型只有内部一致才能有用，同一个模型中不应该有不一致的概念和词汇。</p>

<p>要产出一个大一统的领域模型非常困难和昂贵，甚至是不可能的。Evans根据内部一致的子集来划分领域，称为“界限上下文”。</p>

<p>最近在跟航空业的客户合作时，我们讨论了几个他们业务的核心概念。比如“预定”，我们能在相关业务中找出十七个不同的定义，显然它们不能看做是同一个概念。相反，每个定义都是有细微差异的不同概念。这成为了组织的瓶颈。</p>

<p>界限上下文允许我们在组织范围保留不一致的定义，同时在单个上下文里面保持一致。</p>

<p>我们开始识别哪些能够内部一致的领域模型片段。我们在这些片段的周围画出边界，这就成了我们界限上下文。这样我们就能把我们的业务能力团队对应到界限上下文，这些团队将产出对应的微服务。</p>

<p>微服务的定义又指导了需要哪些十二因子应用。十二因子主要是技术上的规范，而微服务主要是业务上的规范。我们定义界限上下文，赋予之业务能力，围绕业务能力建立团队，让团队开发十二因子应用。由于这样的应用都是独立可部署的，使得业务能力团队有更多的技术手段可用。</p>

<p>我们将界限上下文与“每服务每数据库”模式关联，也就是每个微服务封装、管理和保护它们自己的领域模型和数据存储。在这个模式中，只有一个应用服务可以访问它的数据存储（可能是一个多租户数据库集群中的一个schema，也可以是一个独占的物理数据库）。任何外部访问只能通过API合同（经常是REST，但是可以是任何协议）。</p>

<p>这种分解使应用可以根据自己的数据特征选择不同的数据存储，比如数据结构和数据读写模式。另一方面，为了能回答一些跨上下文的问题，数据通过事件驱动技术重新组合起来。比如<a href="http://martinfowler.com/bliki/CQRS.html">CQRC</a>和<a href="http://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a>就常常用来实现跨上下文将相似概念同步起来。</p>

<h3><a name="c"></a>容器化</h3>

<p>容器镜像（比如由<a href="https://linuxcontainers.org">LXC</a>、<a href="https://www.docker.com">Docker</a>、<a href="https://github.com/coreos/rkt">Rocket</a>准备的）正在迅速成为云架构的部署单元。这些镜像也迅速得到了调度工具的支持，比如Kubernetes、Marathon、 Lattice。公有云提供商比如Amazon和Google也提供了基于容器的调度和部署服务。容器技术提供了跟虚拟机相似的资源分配和隔离，同时相对来说大大地轻量和易移植。应用开发者要尽快适应将应用打包为容器镜像，以便能享有现代云基础设施带来的便利。</p>

<h3>从奏乐到舞蹈<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup></h3>

<p>不仅仅服务产出、数据模型和管理应该去中心化，服务的集成也应该去中心化。企业中服务集成传统上使用类似ESB。ESB就成为了服务间交互的所有决策的控制者，包括路由、传输、协议、安全等。我们称为“奏乐”，类似于指挥决定了整个音乐的演奏进程。ESB和奏乐模式使得架构图显得非常简单，但不幸的是这种简单性具有欺骗性。ESB中经常隐藏着一张复杂性的网。管理这种复杂性非常费时，跟它一起工作也成为了应用开发团队的瓶颈。就像我们谈到的大一统的数据模型一样，大一统的服务集成也成为了追求快速的绊脚石。</p>

<p>在云架构中，比如微服务，我们倾向于“舞蹈”，类似于芭蕾舞。不是把精力放在集成机制，而是放在各个节点。当舞台上发生了跟原计划不符的异常情况，并没有一个指挥来告诉舞者该怎么做，而是舞者自己适应。类似的，我们的服务也是自己适应他们运行环境中出现的异常情况，比如通过“客户端负载均衡<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>”和“断流器<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>”。</p>

<p>虽然架构图看上去趋向于一张纠结的网，但其复杂性并不超过传统的SOA。舞蹈模式只是承认和暴露了系统本质的复杂性。这个转变同样是为了支持自治进而实现云架构快速地目标。团队可以迅速应对各种情况，不必与其他团队过多协作，也不用疲于跟ESB打交道。</p>

<h2>总结</h2>

<p>以下为译者自己总结的 :&ndash;)</p>

<ol>
<li>文化转变：DevOps、持续交付、自治</li>
<li>组织结构转变：业务能力团队、平台团队</li>
<li>技术转变：分解服务、分解数据、容器化、分解集成</li>
</ol>


<p>（<a href="http://nielinjie.github.io/blog/2015/06/07/cloud5/">后文继续，迁移指南</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>business capability<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>bounded contexts，在“<a href="#dd">分解数据</a>”中讨论<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/#ai">前面已经提到</a>。<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>Monolith<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p><a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/#ms">微服务</a>。<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>书：<em>Domain-Driven Design</em>，作者Eric Evans<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
<li id="fn:7">
<p>从Orchestration到Choreography<a href="#fnref:7" rev="footnote">&#8617;</a></p></li>
<li id="fn:8">
<p><a href="http://nielinjie.github.io/blog/2015/05/07/cloud6/#rlb">路由和负载均衡</a><a href="#fnref:8" rev="footnote">&#8617;</a></p></li>
<li id="fn:9">
<p><a href="http://nielinjie.github.io/blog/2015/06/07/cloud6/#ft">错误容忍</a><a href="#fnref:9" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（三）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/05/29/cloud3/"/>
    <updated>2015-05-29T11:15:31+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/05/29/cloud3</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/">接前文</a>）</p>

<h1>需要转变</h1>

<h2>文化转变</h2>

<h3>从竖井到DevOps</h3>

<p>企业IT常常被组织为一些竖井：软件开发、QA、DBA、系统管理、运营、发布管理、项目管理等。这些竖井往往有不同的管理层次、工具集、沟通方式、词汇和激励形式。这些不同鼓励了做事方法的不同。</p>

<!--more-->


<p>一个常被提到的例子是开发和运营的不同。
开发的任务是开发功能，往往是通过向企业IT中引入更改来实现的。所以开发部门的任务可以说成是“交付变化”，自然开发部门也往往是鼓励交付更多的变化。</p>

<p>另一方面，运营的任务则可以说是“防止变化”，运营的任务是保持可用性、弹性、性能和持久性。他们往往被鼓励保持KPI，比如平均错误间隔，平均恢复时间等。然而对于保持这些指标来说，最大的风险就是引入变化。结果就是，运营经常不是想办法怎么安全地引入开发期望的变化，而是想办法把让引入变化更痛苦，从而降低变化的频率。</p>

<p>这种不同的做事方法显然带来了低效的合作。协作、沟通、交接工件变得非常乏味和痛苦，甚至是混乱和危险。企业IT往往企图修复这种状况，构建了通过ticket系统和会议驱动的重流程。结果是工作变得更加慢和浪费了。</p>

<p>像这样的环境跟云架构快速的理念完全相左。竖井诞生的原因，往往是希望带来更多地稳定和安全，但实践证明，作用甚微，甚至有时候会带来反作用。</p>

<p>DevOps的核心就是打破这样的竖井。构建共享的工具、词汇和沟通结构，建立文化，为着同一个目标：快速安全地实现价值。围绕这一目标建立激励机制。官僚机构和流程被信任和责任心取代。</p>

<p>届时，开发和运营向同一个直接领导负责，共同寻找办法，既能持续产出价值，又能保持可用性、弹性、性能和持久性。现在发现，云架构相关技术能够为之提供支持。</p>

<h3>从Punctuated Equilibrium<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> 到持续交付</h3>

<p>企业往往引入了敏捷过程，比如Scrum，但一般限于开发团队。</p>

<p>我们已经相当成功地将单个的开发团队变得敏捷。我们做了很多改进：结对编程、测试驱动开发，持续集成……，我们已经能很高效地完成“从用户故事到Demo”的循环。但我们的问题是：<em>功能什么时候能上生产呢？</em></p>

<p>这个问题我们很难回答，它迫使我们考虑一些我们不能控制的因素：</p>

<ul>
<li>QA流程需要多少时间？</li>
<li>我们啥时候能加入版本计划？</li>
<li>什么时候运营能准备好环境？</li>
</ul>


<p>这时候我们意识到我们陷入了“<a href="http://sdtimes.com/analyst-watch-water-scrum-fall-is-the-reality-of-agile/">Water-Scrum-fall</a>” ，我们的团队已经开始敏捷了，但我们的组织还没有。结果，我们每个迭代的成果并不能体现到生产部署，代码其实还堆积在传统的发布循环中。</p>

<p>这种工作模式实际上抹杀了敏捷的两个关键好处。</p>

<ul>
<li>客户仍然要隔好几个星期才能看到新东西，感觉不到敏捷的好处，所以并不能如约与开发团队建立信任关系。他们倾向于回到老的工作方式：把尽量多的需求都塞到版本里——由于他们不太相信软件能够快速发布，于是宁愿最后好不容易发布的时候能多交付点东西。</li>
<li>开发团队可能要好几周才能得到真正的反馈。Demo是很不错，但开发人员都知道，只有真正的用户使用了真正的生产软件，才能产生好的反馈。这样的反馈才能提供有效的修正，团队才能真正做到“build the right thing”。如果反馈延迟了，错误堆积起来，返工的成本就很高。</li>
</ul>


<p>想要得到云架构的好处就需要转变到持续发布。</p>

<p>技术上我们可以做到每个迭代（甚至每次代码提交）自动部署到生产。我们建立部署管道，在此执行自动测试，防止生产问题。现在唯一需要做的是业务决策：现在是否是发布新特性的好的业务时机？由于部署管道是全自动的，业务决策好后可以一键实施。</p>

<h3>从中央集权到分散自治</h3>

<p>Water-Scrum-fall文化中有一点值得特别指出，因为我感觉是使用云的过程中的一个绊脚石。</p>

<p>企业经常围绕应用架构和数据管理建立中央集权组织，负责维护守则和标准、审批设计和更改。中央集权确实有几点帮助：</p>

<ul>
<li>可以防止技术栈的不一致。降低组织的维护总成本。</li>
<li>可以防止架构选择的不一致。</li>
<li>合规管理之类的Cross-cutting concerns可以在组织内保持一致。</li>
<li>数据的所有权可以一目了然。</li>
</ul>


<p>这些组织建立起来是为了提高质量和降低成本。但收效甚微。同时还影响了云架构在快速方面的努力。单体架构可能成为技术创新的瓶颈，巨大单一的管理组织也一样。有时候一个小的变化只需要几分钟就能完成，而且也肯定会通过评审，但却需要很长时间来等待评审会议的召开。</p>

<p>使用云架构几乎总是伴随着去中心集权。开发云架构应用的团队（“Business Capability Teams”<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>）几乎拥有了交付所需要的所有的能力。他们拥有数据管理、技术栈、架构、每个部件的设计以及API合同。如果有什么决定需要做，那就是这个团队就能做了。</p>

<p>去中心和自治的团队通过集成模式规定的机制来制衡，这些机制存在于团队开发的服务之间，通常很小很轻。（比如使用HTTP REST JSON风格的API，而不是各不相同的RPC接口。）这些机制通常最先出现在基层，用来解决一些共有问题，比如“错误容忍”。我们鼓励各团队自己设计解决方案，然后自组织地与其他团队一起形成公用的模式和框架。如果出现了对整个组织都好用的模式和框架，那么就将这些模式、框架转交给“云工具、框架团队”。工具框架团队可以是平台团队<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>的一部分，也可以不是。在整个团队对架构形成共识的过程中，工具框架团队和他们的方案，将起到带头作用。</p>

<p>（<a href="http://nielinjie.github.io/blog/2015/06/02/cloud4/">后文继续，组织转变</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://en.wikipedia.org/wiki/Punctuated_equilibrium">Punctuated Equilibrium</a>（什么鬼……）<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><a href="http://nielinjie.github.io/blog/2015/06/02/cloud4/#bct">业务能力团队</a><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="http://nielinjie.github.io/blog/2015/06/02/cloud4/#pt">平台团队</a><a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（二）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/05/23/cloud2/"/>
    <updated>2015-05-23T11:15:31+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/05/23/cloud2</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/05/23/cloud/">接前文</a>）</p>

<h2>云架构的定义</h2>

<h3><a name="12f"></a>十二因子应用</h3>

<p>“十二因子应用”是一些云架构模式。它们专注于速度、稳定、伸缩，主要包括强调声明性配置、无状态和无共享的横向伸缩进程、全面与部署环境解耦合等方面。当前大量的云平台都为部署十二因子应用优化。</p>

<p>在“十二因子应用”的说法中，“应用”指的是单个部署单元。</p>

<p>十二因子应用表述为：<br/>
（省略，可参见官方中文版：<a href="http://12factor.net/zh_cn/">十二因子应用</a>）</p>

<!--more-->


<p>十二因子让应用可以快速部署，因为对环境没有多少要求。对环境没有要求也让云平台可以采用简单一致的机制来自动化地提供新的环境。这是十二因子应用在速度方面的意义。</p>

<p>十二因子也使得应用满足“生命短促”的要求，换句话说我们可以以很小的成本丢弃这些应用。首先应用环境本身完全是可丢弃的，同时应用的状态，不管内存中的还是持久化的，都外化到支持服务中。这就使得应用可以简单地自动化地伸缩。在大多数情况下，平台只要按想要的数量简单拷贝现有的环境然后启动进程就可以了。当需要缩减的时候，只要关闭运行中的进程，然后删除相应地环境就可以了，完全不用备份或者保护这些环境。这是十二因子应用在伸缩方面的意义。</p>

<p>最后，应用的可丢弃性使得平台自动错误恢复非常容易快速。同时，将日志作为事件流大大地帮助了应用状态的可视化。环境的等同、一致的配置机制以及支持服务机制则使得平台能为应用运行的各个方面提供更强大的可视化能力。这是十二因子应用在稳定方面的意义。</p>

<h3><a name="ms"></a>微服务</h3>

<p>微服务代表了将单体架构<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>业务系统解构为独立可部署的服务。每个服务代表一个业务能力，或者说是产生业务价值的最小原子服务。</p>

<ul>
<li>我们将业务领域解耦为独立可部署的上下文的同时，我们就解耦了相关的变化迭代。当变化被局限在单个的上下文，这些变化可以独立于业务的其他方面。结果就是可以更频繁迅速地落实，从而持续不断地产生价值。</li>
<li>便于加速开发。包括可以投入更多地开发人员。可以将开发人员分开，让他们平行工作，不用过多的交流协作。</li>
<li>新加入的开发人员上手更快，因为他们需要学习的东西更少，需要打交道的团队也更小。</li>
<li>应用新技术可以更容易。在单体架构应用中应用新技术风险较大，一旦犯错可能拖累整个企业架构。</li>
<li>微服务带来独立有效的伸缩。巨大架构应用也可以伸缩，但要求所有部件一起伸缩，不光是哪些负载重的部件。</li>
</ul>


<h3><a name="ai"></a>敏捷的自助基础设施</h3>

<p>云架构应用的部署与维护往往由开发团队负责。支持自助的平台往往有帮助。</p>

<p>好的平台能为用户建立一个抽象层。在IAAS中，我们通过调用API来建立虚拟服务器、网络、存储，然后应用不同形式的配置和自动化来运行我们的应用和支持服务。平台现在鼓励我们以应用和支持服务的角度思考。</p>

<p>应用代码只是简单地被push到git仓库，平台就开始自动编译打包、建立应用环境、部署应用、启动必要地进程。团队不需要知道代码在哪里运行，也不用知道代码是怎么到那里去的。</p>

<p>支持服务也采用类似的机制。不论需要数据库、消息队列还是邮件服务器，只需要告诉平台你的需求。现在的平台大多提供SQL/NoSQL数据库、消息队列、搜索引擎、缓存等各种重要的支持服务。这些服务的实例可以被绑定到我们应用，必要地使用许可被自动注入到应用中。完全不需要繁琐和容易出错的各种定制。</p>

<p>这些平台还提供各种其他能力：</p>

<ul>
<li>应用实例自动化和按需的伸缩。</li>
<li>应用健康管理。</li>
<li>动态路由和负载均衡。</li>
<li>日志和测量数据的汇总。</li>
</ul>


<p>这一系列的工具保障了团队可以敏捷地开发和运营他们的服务，并且做到快速、稳定、可伸缩。</p>

<h3>基于API的协作</h3>

<p>云架构中，服务之间唯一的交互方式是API，公开发布和有版本的API。通常采用HTTP上的REST风格并使用JSON序列化，但其他协议和序列化方式也完全可以。</p>

<p>团队可以在需要的时候部署新的功能。只要他们不破坏任何既有的API合同，就不用跟其他团队协调同步。跟自助基础设施交互的主要方式也是API，不论新建、伸缩还是维护应用基础设施，都通过API的调用进行。</p>

<p>通过<a href="http://martinfowler.com/articles/consumerDrivenContracts.html">客户驱动合同</a>，交互双方校验合同。服务的消费者不允许访问依赖的实现细节，也不能直接访问它的数据存储。事实上，只有一个服务可以访问它自己的数据存储。这种强制解耦合有利于云架构的速度。</p>

<h3>“反脆弱”</h3>

<p>“反脆弱”并不是指的鲁棒性或弹性。而是指系统在压力下变得更强的特性。什么系统可以做到这个？比如人的免疫系统，暴露的时候变得更强而隔离的时候变得更弱。比如，Netflix Simian Army项目有个Chaos Monkey子模块，通过向生产部件中随机注入错误来寻找架构中的弱点。通过显式地寻找弱点、注入错误、强制修复，架构自然随时间逐渐变强。</p>

<h2>总结</h2>

<p>以下为译者自己总结的 :&ndash;)</p>

<ol>
<li>提出创新时代的四个问题：速度、稳定、伸缩、移动</li>
<li>云架构在这四个问题上大致有何思路。</li>
<li>针对四个问题，云架构有五个措施：十二因子、微服务、自助基础设施、基于API协作、反脆弱</li>
</ol>


<p>（<a href="http://nielinjie.github.io/blog/2015/05/29/cloud3/">后文继续，需要转变</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>monolithic application architectures<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[译：迁移至云架构（一）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/05/23/cloud/"/>
    <updated>2015-05-23T11:15:31+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/05/23/cloud</id>
    <content type="html"><![CDATA[<p>原文：<a href="http://pivotal.io/platform-as-a-service/migrating-to-cloud-native-application-architectures-ebook">Migrating to Cloud-Native Application Architectures</a></p>

<p>本文为“节选译”，不是逐句逐段翻译，是意译和无废话译。</p>

<h1>云架构的兴起</h1>

<p>创新型公司遇到的问题是：</p>

<ul>
<li>创新的速度</li>
<li>总是可用的服务</li>
<li>Web的伸缩</li>
<li>移动为中心的用户体验</li>
</ul>


<!--more-->


<p>迁移到云是一个自然地选择，”云架构“是这些公司获得搅局能力的关键。</p>

<p>“云”指的是：任何计算环境，其中的资源（比如计算、网络、存储等）可以随时按需、自助地提供和释放。</p>

<h2>为什么需要“云架构”？</h2>

<h3>速度</h3>

<p>互联网公司声称每天部署几百次，为何需要频繁部署？如果你可以每天部署几百次，那么你可以几乎实时纠正错误；如果你可以实时纠正错误，那你可以进行更多的试错；如果你可以更多的试错，那你可以进行更广泛地实验。这很可能带来下一个竞争优势。</p>

<p>云基础设施灵活和自助的特性正好满足要求。就提供一个新的应用环境来说，调用一个云服务的API当然比传统流程快得多。再加上可以在持续集成\构建环境中加入钩子和其他联动，可以进一步加快速度。</p>

<h3>稳定</h3>

<p>云架构平衡了快速和稳定、可用、持久。</p>

<p>前面已经提到，云架构提供了迅速纠正错误的能力。注意，这里<strong>不是指防止错误</strong>。</p>

<p>那么我们如何即快速又稳定呢？</p>

<ul>
<li><p>可视化<br/>
我们的架构必须提供工具来及时发现失败。我们需要监测整个系统：定义一个”正常状态“，及时发现偏差，并能找到导致偏差的部件。功能丰富的测量、监控、报警、数据可视化框架和工具是云架构的重要组成部分。</p></li>
<li><p>缺陷隔离<br/>
为了控制失败带来的风险，我们需要限制被失败影响的部件或者特性。如果仅仅因为推荐引擎失效就导致所有人不能买东西，那肯定是灾难性的。单体架构的系统往往是这种情形。云架构系统经常采用”微服务“<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>，通过采用微服务构建系统，我们可以把失败限制在单个微服务中。当然，需要”缺陷容忍“特征的配合。</p></li>
<li><p>缺陷容忍<br/>
将系统解构为独立部署的部件还不够。我们还必须防止一个部件中的失败在部件的依赖者中变成一个连环失败。Mike Nygrad描述了<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>几种缺陷容忍的模式，其中最常用的是”断流器“。软件断流器跟电路断流器非常相似：通过断开部件之间的连接来阻止连环失败的产生发展。断流器往往还能在断开的时候提供合适的默认行为。后续会进一步讨论。<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p></li>
<li>自动恢复<br/>
如果发现某些地方有问题，那我们一般简单重启或者重新部署相关的服务。云架构一般不需要人工干预，我们采用自动发现和恢复。<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></li>
</ul>


<h3>伸缩</h3>

<p>当需求增长，我们需要伸展我们的容量来服务需求。以前我们更多地采用纵向伸展：买更强大的服务器。</p>

<p>创新公司使用两个创新性的办法来解决问题：</p>

<ul>
<li>不买强大的服务器，而是把应用实例横向伸展到大量的便宜机器上去。这些机器容易得到而且到位很快。</li>
<li>将没有充分利用的强大服务器虚拟化成多个小服务器，在上面部署独立的负载。</li>
</ul>


<p>当公用云出现，这两个办法得到了发展：虚拟化方面由云供应商来处理，用户专心处理横向伸缩。最近另一个趋势出现：作为应用部署的单元，虚拟服务器正在往“容器”转变。后续进一步讨论。<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></p>

<p>这种转变进一股降低了创新门槛。部署和维护软件的成本都降低了。迎合需求改变软件的速度也非常快。</p>

<p>这些收益的代价就是，我们必须面向横向伸缩来架构我们的应用。云的灵活性要求“生命短促”，不仅要能快速建立应用实例，还必须能快速安全地销毁实例。这是一个“状态管理”的问题，可销毁和可持续性的关系如何？传统的方法比如“集群会话”和“共享文件系统”都不能很好地伸缩。</p>

<p>所以云架构的另一个特点是状态外化，将状态交给外部状态管理服务（比如数据网格、缓存、对象存储等），同时保持应用实例本身是“无状态”的。无状态的应用实例可以快速地创建和销毁，跟外部状态管理服务也可以快速连接和分开，这样就可以快速响应需求的变化。当然这也要求外部状态管理服务本身是可以伸缩的。多数的云服务商已经意识到这一点，提供了很健壮的类似服务。</p>

<h3><a name="mc"></a>移动应用和客户端多样性</h3>

<p>应用需要”随时随地“满足需求。访问成几何级数增长。云架构予以支持。</p>

<p>移动平台是多样化的，移动应用经常需要跟遗留服务或者云中的微服务打交道。这些服务不可能满足各种移动平台的用户的不同需求。将整合多样性的任务交给移动开发会增加网络延迟和流量，带来响应慢和电池消耗。云架构可以将整合任务放到服务器端，比如通过模式”API网关&#8221;。后续进一步讨论。<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup></p>

<p>（<a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/">后文继续，云架构的定义</a>）</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/#ms">微服务</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>书：<em>Release It!</em><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="http://nielinjie.github.io/blog/2015/06/07/cloud6/#bc">断流器</a><a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>要求微服务的一些特性，见<a href="http://nielinjie.github.io/blog/2015/05/23/cloud2/#12f">“十二因子应用”</a>。<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p><a href="http://nielinjie.github.io/blog/2015/06/02/cloud4/#c">容器化</a><a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p><a href="http://nielinjie.github.io/blog/2015/06/07/cloud6/#api">API网关和边沿服务</a><a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[首席程序员负责制]]></title>
    <link href="http://nielinjie.github.io/blog/2015/04/25/chief-programmer/"/>
    <updated>2015-04-25T13:40:12+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/04/25/chief-programmer</id>
    <content type="html"><![CDATA[<p>&ldquo;首席程序员负责制”是我常向团队推荐的一种敏捷实践。</p>

<!--more-->


<ul>
<li>主要用于成员构成比较复杂的团队。比如编程技能参差不齐、领域熟悉程度不同、责任心和士气有待提升、<s>帮派</s>（大误）等情况。</li>
<li>只有一部分程序员有权力提交代码，称为“首席程序员”，他们对所以提交的代码负责。</li>
<li>其他程序参与除提交代码以外的所有活动。他们产出的代码交给所属的首席程序员处理。</li>
<li>其他程序员合适的时候可以转变为首席程序员。</li>
</ul>


<p><img src="http://nielinjie.github.io/images/chief/chief.JPG" width="300"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[质量保障措施参考清单（三）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/04/10/resilient3/"/>
    <updated>2015-04-10T10:18:14+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/04/10/resilient3</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/04/10/resilient2/">接上篇</a>）</p>

<h2>业务正确性和容错性</h2>

<p>系统不仅需要业务功能结果正确，还需要在业务发生问题的时候能够尽量多的提供诊断和恢复支持。</p>

<!-- more -->


<h3>设计时</h3>

<p>设计时可以识别的设计要求大致有：</p>

<ul>
<li>业务错误可以回查。</li>
<li>业务过程可以回查。</li>
<li>业务操作可以回查。</li>
<li>系统故障情况下业务（一定程度上）可以持续。</li>
</ul>


<p>为满足此要求，需要：</p>

<ul>
<li>定义业务过程、业务状态。</li>
<li>记录业务过程和状态的变化。</li>
<li>为各个状态定义业务备案。</li>
<li>记录业务操作。</li>
</ul>


<h3>业务接入时</h3>

<ul>
<li>业务被误用。

<ul>
<li>明确的业务定位</li>
</ul>
</li>
<li>系统之间业务状态不正确或不明确。

<ul>
<li>记录业务进出口状态。</li>
</ul>
</li>
</ul>


<h3>日常趋势</h3>

<ul>
<li>事故趋势恶化

<ul>
<li>业务逻辑有问题？</li>
<li>用户体验有问题？

<ul>
<li>用户体验数据</li>
</ul>
</li>
<li>需要事故记录</li>
</ul>
</li>
</ul>


<h3>事故时</h3>

<ul>
<li>系统操作中断，比如可用性事故引起。

<ul>
<li>需要各个状态定义业务备案。</li>
</ul>
</li>
<li>事后发现业务结果不正确。

<ul>
<li>记录业务过程状态变化。</li>
</ul>
</li>
<li>系统错误

<ul>
<li>大范围错误 &ndash; 归结为<a href="http://nielinjie.github.io/blog/2015/04/10/resilient2/">可用性事故</a>。</li>
<li>局部错误 &ndash; 归结为bug。</li>
<li>需要系统错误率实时监控和报警。</li>
<li>需要系统错误持续记录和分析。</li>
</ul>
</li>
</ul>


<h3>小结</h3>

<p>综上，为持续保障系统正确性，需要采取的措施有：</p>

<ul>
<li><strong>明确的业务定位。</strong>以及定期review。</li>
<li><strong>定义各个业务过程、业务状态。</strong></li>
<li><strong>业务过程和业务状态的变化的记录。</strong></li>
<li><strong>各个状态定义业务备案。</strong></li>
<li><strong>业务操作的记录。</strong></li>
<li><strong>业务进出口状态的记录。</strong></li>
<li><strong>用户体验数据持续记录和分析。</strong></li>
<li><strong>系统错误率持续监控和阙值报警。</strong></li>
<li><strong>系统错误持续记录和分析。</strong></li>
<li><strong>事故记录与分析。</strong></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[质量保障措施参考清单（二）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/04/10/resilient2/"/>
    <updated>2015-04-10T10:18:14+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/04/10/resilient2</id>
    <content type="html"><![CDATA[<p>（<a href="http://nielinjie.github.io/blog/2015/04/09/resilient/">接上篇</a>）</p>

<h2>可用性</h2>

<p>可用性风险通常比较被重视，企业默认架构中往往有较多应对措施。而且很多可用性风险与<a href="http://nielinjie.github.io/blog/2015/04/09/resilient/">性能与容量风险</a>有关，所以这里总结的风险和保障措施显得比较少。</p>

<!-- more -->


<h3>设计时</h3>

<p>企业默认架构中一般包含可用性的基本应对：</p>

<ul>
<li>HA方案，比如热备冷备及相关切换方案等。</li>
<li>容灾方案</li>
</ul>


<h3>业务接入时</h3>

<p>一般不会产生可用性风险。除非超过系统容量造成系统不可用。这个风险<a href="http://nielinjie.github.io/blog/2015/04/09/resilient/">在“性能与容量”部分讨论</a>。</p>

<h3>日常趋势</h3>

<ul>
<li>事故趋势恶化。</li>
<li>资源消耗异常。（<a href="http://nielinjie.github.io/blog/2015/04/09/resilient/">在“性能与容量”部分讨论</a>。）</li>
</ul>


<h3>事故时</h3>

<p>可用性事故一般有：</p>

<ul>
<li>系统失去响应

<ul>
<li>容量问题造成的。（<a href="http://nielinjie.github.io/blog/2015/04/09/resilient/">在“性能与容量”部分讨论</a>。）</li>
<li>某个（某些）节点失去响应造成。

<ul>
<li>需要各节点可用性数据。</li>
<li>节点可拔插。</li>
</ul>
</li>
</ul>
</li>
<li>系统大范围报错

<ul>
<li>诊断用现场实时数据。（<a href="http://nielinjie.github.io/blog/2015/04/09/resilient/">在“性能与容量”部分讨论</a>。）</li>
</ul>
</li>
</ul>


<p>为及时发现可用性事故，我们需要：</p>

<ul>
<li>可用性数据实时监控和报警</li>
</ul>


<h3>小结</h3>

<p>综上，为持续保障系统可用性，需要采取的措施有：</p>

<ul>
<li><strong>预先设计的HA和容灾方案。</strong></li>
<li><strong>事故记录与分析。</strong></li>
<li><strong>可用性数据持续监控与阙值报警。</strong></li>
<li><strong>各节点可用性实时数据。</strong></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“服务化”]]></title>
    <link href="http://nielinjie.github.io/blog/2015/04/09/service/"/>
    <updated>2015-04-09T11:11:07+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/04/09/service</id>
    <content type="html"><![CDATA[<p>“服务化”，就是既有系统进化为“服务”的努力。</p>

<p>什么是服务？每个架构师都有不同的定义，在不同的场景下可能也有不同的定义。</p>

<!-- more -->


<p>在各系统“服务化”的场景中，我们对“服务”的定义是：</p>

<blockquote><p><strong>自管理的业务能力。</strong></p></blockquote>

<p>这个定义里有两个要点：</p>

<ol>
<li>自管理。自管理意味着服务自身对服务内容和服务质量负责。而不是用户负责或者双方共同负责。同时，也意味着对服务内容和质量有定义和进化的权利。</li>
<li>业务：服务合同以业务为依据制定。比如：

<ul>
<li>服务内容，只包括某领域业务范围的服务，而不传递其他领域的服务。</li>
<li>服务粒度，以业务场景为依据划分服务和设计交互。</li>
<li>服务形式，以最大限度支持业务的变化为依据制定。</li>
</ul>
</li>
</ol>


<p>系统“服务化”实践中，我们用三个线索来思考改善这两个要点：</p>

<ol>
<li>系统应该/可以提供何种服务？：

<ol>
<li>系统的业务定位如何？即它在所参与的企业活动中的位置如何？</li>
<li>这种定位是否理想？可能如何进化？</li>
</ol>
</li>
<li>系统以何种形式提供服务？

<ol>
<li>基于所处的业务定位，何种服务形式可以适应业务快速发展？</li>
<li>基于所处的企业IT环境，何种服务形式便于其他系统使用？</li>
</ol>
</li>
<li>系统如何实现（或改善）自管理能力？<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>

<ol>
<li>“服务”对系统各方面质量特别是运行时质量提出了不一样的要求。</li>
<li><a href="blog/filter/filter.html?filterName=series&amp;filterValue=%E8%B4%A8%E9%87%8F%E4%BF%9D%E9%9A%9C%E6%8E%AA%E6%96%BD%E5%8F%82%E8%80%83%E6%B8%85%E5%8D%95&amp;filterDes=Series%3A%20%E8%B4%A8%E9%87%8F%E4%BF%9D%E9%9A%9C%E6%8E%AA%E6%96%BD%E5%8F%82%E8%80%83%E6%B8%85%E5%8D%95">系列：质量保障措施参考清单</a></li>
</ol>
</li>
</ol>


<h4>ps: FAQ</h4>

<p>Q：“服务化”是为了减小系统开发维护成本么？<br/>
A：可能碰巧有这个效果，但目标不是减小单个系统的成本，而是减小企业范围的总成本。减小的是：业务变化时，IT系统适应业务并支持业务所需的成本。</p>

<p>Q：“服务化”是“组件化”么？<br/>
A：“服务化”是使既有既有系统形成适当的“组件”的过程。可以说“服务化”是为了“组件化”。“组件化”包括两个方面：</p>

<ol>
<li>形成适当的“组件”</li>
<li>形成“组件”间有效组合并管理的机制。</li>
</ol>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>主要考虑服务质量部分，服务内容部分已在1.中考虑。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[质量保障措施参考清单（一）]]></title>
    <link href="http://nielinjie.github.io/blog/2015/04/09/resilient/"/>
    <updated>2015-04-09T10:18:14+08:00</updated>
    <id>http://nielinjie.github.io/blog/2015/04/09/resilient</id>
    <content type="html"><![CDATA[<p>为更好的应对业务快速变化带来的挑战，许多系统正在从“关联系统”角色向着“<a href="http://nielinjie.github.io/blog/2015/04/09/service/">服务</a>”角色转变。这样的转变对系统各方面质量特别是运行时质量提出了不一样的要求。如何管理并始终满足这些要求，打造弹性、容错、可管理的服务，是我们需要重点考虑的问题。</p>

<!-- more -->


<p>作为“<a href="http://nielinjie.github.io/blog/2014/03/31/aa1/">风险驱动的架构设计</a>”的实践者，我们以系统运行过程各个阶段可能遇到的风险为线索，整理我们可能采取的保障措施。——</p>

<ul>
<li>设计时</li>
<li>业务接入时</li>
<li>日常趋势</li>
<li>事故时</li>
</ul>


<h2>性能和容量</h2>

<h3>设计时</h3>

<p>需要根据业务目标确定适当性能理想值。比如：</p>

<ul>
<li>响应时间</li>
<li>吞吐量</li>
</ul>


<h3>业务接入时</h3>

<p>新的业务接入，不论是新的系统接入还是旧有接入系统业务量发生变化。</p>

<ul>
<li>新系统接入时应该声明业务量和变化趋势。</li>
<li>接入系统业务量（即将）发生显著变化时应该声明。</li>
<li>需要知晓当前系统可以承受的业务量。</li>
</ul>


<h3>日常趋势</h3>

<p>系统运行中，几个方面的风险需要持续防范：</p>

<ul>
<li>性能表现恶化</li>
<li>业务量异常变化</li>
<li>资源消耗异常变化</li>
</ul>


<p>为此，我们需要：</p>

<ul>
<li>性能指标的周期性监控记录。</li>
<li>业务总量的周期性监控记录。</li>
<li>各接入系统业务量周期性监控记录。</li>
<li>资源消耗量周期性监控记录。</li>
</ul>


<h3>事故时</h3>

<p>性能事故指的是性能表现发生急剧恶化，用户受到明显影响。性能事故的原因可以分两种情况分析——</p>

<ul>
<li>业务量发生变化

<ul>
<li>各接入系统业务量及调用量。</li>
<li>隔离、特别是热隔离。</li>
<li>扩展性，特别是横向扩展。</li>
</ul>
</li>
<li>业务量没有明显变化

<ul>
<li>受依赖系统拖累

<ul>
<li>依赖系统调用记录。</li>
<li>替代方案或后备方案。</li>
</ul>
</li>
<li>某（些）节点性能异常

<ul>
<li>各节点的性能数据。</li>
<li>节点可（热）拔插。</li>
</ul>
</li>
<li>变更 &ndash; 环境、代码

<ul>
<li>诊断用现场数据</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>为及时发现性能事故和区分两种情况，我们需要：</p>

<ul>
<li>性能数据，实时监控和阙值报警。</li>
<li>总业务量，包括时间和功能分布。</li>
</ul>


<h3>小结</h3>

<p>综上，为持续保障系统性能表现，需要采取的措施有：</p>

<ul>
<li><strong>根据业务目标有适当的性能和容量的理想值。</strong>影响因素变化后或者周期性地需要review。</li>
<li><strong>明确当前系统最大承受业务量。</strong>影响因素变化后或者周期性地需要review。</li>
<li><strong>确保新系统接入或接入系统业务量变化时声明业务量和变化趋势。</strong>建立机制，包括事后回朔机制。</li>
<li><strong>系统性能指标的阙值报警。</strong></li>
<li><strong>系统性能指标的持续监控记录和分析。</strong></li>
<li><strong>业务总量的持续监控和分析。</strong></li>
<li><strong>各接入系统业务量持续监控和分析。</strong></li>
<li><strong>资源消耗量持续监控和分析。</strong></li>
<li><strong>各接入系统业务量及调用量实时数据。</strong></li>
<li><strong>有能力实时隔离接入系统。</strong></li>
<li><strong>容量可扩展性，特别是横向扩展。</strong></li>
<li><strong>依赖系统调用记录的实时数据。</strong>包括调用量和性能。</li>
<li><strong>依赖系统替代方案或后备方案。</strong>至少有业务层面后备方案。</li>
<li><strong>各节点的性能指标的实时数据。</strong></li>
<li><strong>各节点可（热）拔插。</strong>拔出后性能和容量可接受。</li>
<li><strong>诊断用现场实时数据。</strong>各节点都需要此类数据，比如线程dump。</li>
</ul>

]]></content>
  </entry>
  
</feed>
